{"cells":[{"cell_type":"code","source":["import requests\n","import json\n","from google.colab import userdata\n","\n","key = userdata.get(\"OPENROUTER_API_KEY\")\n","response = requests.get(\n","  url=\"https://openrouter.ai/api/v1/auth/key\",\n","  headers={\n","    \"Authorization\": f\"Bearer {key}\"\n","  }\n",")\n","\n","print(\"key -> \" + json.dumps(response.json(), indent=2))"],"metadata":{"id":"3qpBiObXY07f"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cDtaQY7pQgLR"},"outputs":[],"source":["import os\n","import json\n","import re\n","import requests\n","import itertools\n","from datetime import datetime, timedelta\n","import pandas as pd\n","import numpy as np\n","from typing import List, Dict, Any, Optional\n","import time\n","import random\n","\n","# =============================================================================\n","# PHASE 0: MODE SELECTION WITH CUSTOM SECTION OPTION\n","# =============================================================================\n","\n","def select_run_mode():\n","    \"\"\"Allow user to select between test mode and full mode\"\"\"\n","    print(\"ðŸš€ SpecSentinel MVP\")\n","    print(\"=\"*50)\n","    print(\"How do you want to run SpecSentinel?\")\n","    print(\"1. Test Mode - Limited specifications, free models only\")\n","    print(\"2. Full Mode - Complete analysis with all specifications\")\n","    print(\"=\"*50)\n","\n","    while True:\n","        try:\n","            choice = input(\"Enter your choice (1 or 2): \").strip()\n","            if choice == '1':\n","                return 'test'\n","            elif choice == '2':\n","                return 'full'\n","            else:\n","                print(\"Please enter either 1 or 2\")\n","        except KeyboardInterrupt:\n","            print(\"\\nExiting...\")\n","            return None\n","\n","def get_custom_section_choice():\n","    \"\"\"Ask user if they want to analyze a custom section in test mode\"\"\"\n","    print(\"\\nðŸ” Test Mode Section Selection\")\n","    print(\"=\"*40)\n","    print(\"Do you want to analyze a specific Java specification section?\")\n","    print(\"1. Yes - I'll specify a custom section\")\n","    print(\"2. No - Use default test sections\")\n","    print(\"=\"*40)\n","\n","    while True:\n","        try:\n","            choice = input(\"Enter your choice (1 or 2): \").strip()\n","            if choice == '1':\n","                return True\n","            elif choice == '2':\n","                return False\n","            else:\n","                print(\"Please enter either 1 or 2\")\n","        except KeyboardInterrupt:\n","            print(\"\\nExiting...\")\n","            return None\n","\n","def get_custom_section_details():\n","    \"\"\"Get custom section details from user\"\"\"\n","    print(\"\\nðŸ“ Enter Custom Section Details\")\n","    print(\"=\"*35)\n","    print(\"Please provide the following information:\")\n","\n","    # Get section title\n","    while True:\n","        title = input(\"Section Title (e.g., 'The switch Statement'): \").strip()\n","        if title:\n","            break\n","        print(\"Please enter a valid section title.\")\n","\n","    # Get section number\n","    while True:\n","        section_num = input(\"Section Number (e.g., '14.11' or '8.4.8'): \").strip()\n","        if section_num and re.match(r'^\\d+(\\.\\d+)*$', section_num):\n","            break\n","        print(\"Please enter a valid section number (e.g., '14.11' or '8.4.8').\")\n","\n","    # Create section key from title\n","    section_key = title.lower().replace(' ', '_').replace('(', '').replace(')', '').replace(',', '')\n","    section_key = re.sub(r'[^a-z0-9_]', '', section_key)\n","\n","    return {\n","        'key': section_key,\n","        'title': title,\n","        'section_number': section_num\n","    }\n","\n","# =============================================================================\n","# PHASE 1: SETUP AND DEPENDENCIES\n","# =============================================================================\n","\n","# Install required packages\n","print(\"ðŸ“¦ Installing dependencies...\")\n","!pip install -q transformers torch\n","!pip install -q sentence-transformers\n","!pip install -q spacy nltk beautifulsoup4 lxml\n","!pip install -q z3-solver sympy\n","!pip install -q openai httpx requests\n","!pip install -q PyPDF2 pdfplumber\n","!pip install -q python-dotenv\n","!pip install streamlit\n","\n","# Download spaCy model\n","!python -m spacy download en_core_web_sm\n","\n","# Import all necessary libraries\n","import spacy\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","from bs4 import BeautifulSoup\n","from z3 import *\n","import openai\n","from google.colab import userdata\n","import pickle\n","import torch\n","from sentence_transformers import SentenceTransformer\n","import streamlit as st\n","\n","\n","# Download NLTK data\n","nltk.download('punkt', quiet=True)\n","nltk.download('punkt_tab', quiet=True)\n","nltk.download('stopwords', quiet=True)\n","\n","print(\"âœ… Dependencies installed successfully!\")\n","\n","# Define project path\n","project_path = '/content/drive/My Drive/SpecSentinel'\n","\n","def setup_project_path(mode='full'):\n","    \"\"\"Setup environment based on mode\"\"\"\n","    print(f\"ðŸš€ SpecSentinel MVP - Setting up project structure ({mode} mode)...\")\n","\n","    # Check if Drive is already mounted\n","    if not os.path.exists('/content/drive'):\n","        print(\"ðŸ“‚ Mounting Google Drive...\")\n","        drive.mount('/content/drive')\n","    else:\n","        print(\"ðŸ“‚ Google Drive already mounted\")\n","\n","    # Check if SpecSentinel folder exists and return early if it does\n","    if os.path.exists(project_path):\n","        print(f\"ðŸ“ Project directory already exists at: {project_path}\")\n","        return project_path\n","\n","    # Create project directory structure\n","    print(\"ðŸ—ï¸ Creating project directory structure...\")\n","    os.makedirs(f'{project_path}/data', exist_ok=True)\n","    os.makedirs(f'{project_path}/models', exist_ok=True)\n","    os.makedirs(f'{project_path}/results', exist_ok=True)\n","    os.makedirs(f'{project_path}/logs', exist_ok=True)\n","\n","    print(f\"âœ… Project directory created at: {project_path}\")\n","\n","    return project_path\n","\n","# =============================================================================\n","# PHASE 2: OPENROUTER API SETUP WITH MODE-AWARE MODEL SELECTION\n","# =============================================================================\n","\n","class EnhancedOpenRouterClient:\n","    def __init__(self, mode='full'):\n","        self.mode = mode\n","\n","        # Get API keys from Colab secrets (supports multiple keys)\n","        self.api_keys = []\n","        try:\n","            # Try to get multiple API keys\n","            for i in range(1, 6):  # Support up to 5 keys\n","                key_name = f'OPENROUTER_API_KEY_{i}' if i > 1 else 'OPENROUTER_API_KEY'\n","                key = os.getenv(key_name)\n","                try:\n","                    key = userdata.get(key_name)\n","                    if key:\n","                        self.api_keys.append(key)\n","                except:\n","                    break\n","\n","            if not self.api_keys:\n","                raise ValueError(\"No API keys found\")\n","\n","        except Exception as e:\n","            # Fallback: Ask user to input API keys\n","            from getpass import getpass\n","            print(f\"No API keys found in secrets. Please enter your OpenRouter API keys:\")\n","            key = getpass(\"Enter your primary OpenRouter API key: \")\n","            self.api_keys.append(key)\n","\n","            # In test mode, we only need one API key for free models\n","            if mode == 'full':\n","                while True:\n","                    additional = input(\"Do you have additional API keys? (y/n): \").lower()\n","                    if additional == 'y':\n","                        key = getpass(\"Enter additional API key: \")\n","                        self.api_keys.append(key)\n","                    else:\n","                        break\n","\n","        self.current_key_idx = 0\n","\n","        # Model selection based on mode\n","        if mode == 'test':\n","            # Test mode: Only free models\n","            self.models = [\n","                \"openai/gpt-4.1-mini\",\n","                \"deepseek/deepseek-chat-v3-0324:free\",\n","                \"meta-llama/llama-3.2-3b-instruct:free\",\n","                \"google/gemma-2-9b-it:free\",\n","                \"microsoft/phi-3-mini-128k-instruct:free\"\n","            ]\n","            print(\"ðŸ§ª Test mode: Using free models only\")\n","        else:\n","            # Full mode: model hierarchy (most powerful to least powerful)\n","            self.models = [\n","                # Most Powerful Models\n","                \"openai/gpt-4.1-mini\",\n","                \"anthropic/claude-3.5-haiku\",\n","                \"google/gemini-2.5-flash-preview-05-20\",\n","                \"google/gemma-3-12b-it\",\n","                \"meta-llama/llama-4-maverick\",\n","                # Free Models\n","                \"deepseek/deepseek-chat-v3-0324:free\",\n","                \"meta-llama/llama-3.2-3b-instruct:free\",\n","                \"google/gemma-2-9b-it:free\",\n","                \"microsoft/phi-3-mini-128k-instruct:free\"\n","            ]\n","            print(\"ðŸš€ Full mode: Using all available models\")\n","\n","        self.current_model_idx = 0\n","        self.model_cooldowns = {}  # Track cooldown periods for models\n","        self.model_error_counts = {}  # Track error counts per model\n","\n","    def get_current_api_key(self) -> str:\n","        \"\"\"Get current API key with rotation\"\"\"\n","        return self.api_keys[self.current_key_idx % len(self.api_keys)]\n","\n","    def rotate_api_key(self):\n","        \"\"\"Rotate to next API key\"\"\"\n","        self.current_key_idx = (self.current_key_idx + 1) % len(self.api_keys)\n","\n","    def is_model_in_cooldown(self, model: str) -> bool:\n","        \"\"\"Check if model is in cooldown period\"\"\"\n","        if model not in self.model_cooldowns:\n","            return False\n","\n","        cooldown_until = self.model_cooldowns[model]\n","        return datetime.now() < cooldown_until\n","\n","    def set_model_cooldown(self, model: str):\n","        \"\"\"Set cooldown period for model based on whether it's free or paid\"\"\"\n","        is_free_model = ':free' in model\n","        # Longer cooldowns in test mode to be more conservative with free models\n","        if self.mode == 'test':\n","            cooldown_seconds = 15 if is_free_model else 90\n","        else:\n","            cooldown_seconds = 10 if is_free_model else 60\n","\n","        self.model_cooldowns[model] = datetime.now() + timedelta(seconds=cooldown_seconds)\n","        print(f\"â³ Model {model} in cooldown for {cooldown_seconds} seconds\")\n","\n","    def call_llm(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.3) -> str:\n","        \"\"\"LLM calling with improved error handling and API key rotation\"\"\"\n","        available_models = [m for m in self.models if not self.is_model_in_cooldown(m)]\n","\n","        if not available_models:\n","            print(\"âš ï¸ All models in cooldown, waiting...\")\n","            wait_time = 15 if self.mode == 'test' else 5\n","            time.sleep(wait_time)\n","            available_models = self.models\n","\n","        for model in available_models:\n","            for attempt in range(len(self.api_keys)):\n","                try:\n","                    current_key = self.get_current_api_key()\n","\n","                    response = requests.post(\n","                        \"https://openrouter.ai/api/v1/chat/completions\",\n","                        headers={\n","                            \"Authorization\": f\"Bearer {current_key}\",\n","                            \"Content-Type\": \"application/json\",\n","                            \"HTTP-Referer\": \"https://github.com/your-repo\",\n","                            \"X-Title\": \"SpecSentinel\"\n","                        },\n","                        json={\n","                            \"model\": model,\n","                            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n","                            \"max_tokens\": max_tokens,\n","                            \"temperature\": temperature\n","                        },\n","                        timeout=30\n","                    )\n","\n","                    if response.status_code == 200:\n","                        result = response.json()\n","                        return result['choices'][0]['message']['content']\n","                    elif response.status_code == 429:  # Rate limit\n","                        print(f\"ðŸ”„ Rate limit for {model} with key {attempt+1}, rotating...\")\n","                        self.rotate_api_key()\n","                        wait_time = 5 if self.mode == 'test' else 2\n","                        time.sleep(wait_time)\n","                        continue\n","                    elif response.status_code == 401:  # Auth error\n","                        print(f\"ðŸ”‘ Auth error with key {attempt+1}, rotating...\")\n","                        self.rotate_api_key()\n","                        continue\n","                    else:\n","                        print(f\"âŒ Model {model} failed with status {response.status_code}\")\n","                        break\n","\n","                except requests.exceptions.Timeout:\n","                    print(f\"â° Timeout for {model}, trying next...\")\n","                    break\n","                except Exception as e:\n","                    print(f\"âŒ Model {model} failed: {str(e)[:100]}\")\n","                    break\n","\n","            # Set cooldown for failed model\n","            self.set_model_cooldown(model)\n","\n","        raise Exception(\"All models and API keys failed\")\n","\n","# =============================================================================\n","# PHASE 3: SPECIFICATION DATA ACQUISITION WITH CUSTOM SECTION SUPPORT\n","# =============================================================================\n","\n","class JSONArrayManager:\n","    \"\"\"Utility class for managing JSON files containing arrays.\"\"\"\n","\n","    @staticmethod\n","    def save_json_array(file_path, data, append=True, create_dirs=True):\n","        \"\"\"\n","        Save data to a JSON file containing an array.\n","\n","        Args:\n","            file_path (str): Path to the JSON file\n","            data: Data to save (will be converted to list if not already)\n","            append (bool): If True, append to existing file; if False, overwrite\n","            create_dirs (bool): If True, create directories if they don't exist\n","\n","        Returns:\n","            bool: True if successful, False otherwise\n","        \"\"\"\n","        try:\n","            # Create directories if needed\n","            if create_dirs:\n","                os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","\n","            # Ensure data is a list\n","            if not isinstance(data, list):\n","                data = [data] if data is not None else []\n","\n","            # Check if file exists\n","            if not os.path.exists(file_path):\n","                # File doesn't exist - create new file with original behavior\n","                with open(file_path, 'w', encoding='utf-8') as f:\n","                    json.dump(data, f, indent=2, ensure_ascii=False)\n","                return True\n","\n","            # File exists - handle based on append flag\n","            if not append:\n","                # Overwrite existing file\n","                with open(file_path, 'w', encoding='utf-8') as f:\n","                    json.dump(data, f, indent=2, ensure_ascii=False)\n","                return True\n","\n","            # File exists and append=True - read existing data and append\n","            try:\n","                with open(file_path, 'r', encoding='utf-8') as f:\n","                    existing_data = json.load(f)\n","\n","                # Ensure existing data is a list\n","                if not isinstance(existing_data, list):\n","                    existing_data = [existing_data]\n","\n","            except (json.JSONDecodeError, FileNotFoundError):\n","                # File exists but invalid JSON, treat as new file\n","                existing_data = []\n","\n","            # Append new data to existing data\n","            existing_data.extend(data)\n","\n","            # Write combined data back to file\n","            with open(file_path, 'w', encoding='utf-8') as f:\n","                json.dump(existing_data, f, indent=2, ensure_ascii=False)\n","\n","            return True\n","\n","        except Exception as e:\n","            print(f\"Error saving JSON array to {file_path}: {e}\")\n","            return False\n","\n","    @staticmethod\n","    def save_single_object(file_path, obj, create_dirs=True):\n","        \"\"\"\n","        Save a single object to a JSON file (not as an array).\n","\n","        Args:\n","            file_path (str): Path to the JSON file\n","            obj: Single object to save (dict, string, number, etc.)\n","            create_dirs (bool): If True, create directories if they don't exist\n","\n","        Returns:\n","            bool: True if successful, False otherwise\n","        \"\"\"\n","        try:\n","            # Create directories if needed\n","            if create_dirs:\n","                os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","\n","            # Save the single object directly (not wrapped in an array)\n","            with open(file_path, 'w', encoding='utf-8') as f:\n","                json.dump(obj, f, indent=2, ensure_ascii=False)\n","\n","            return True\n","\n","        except Exception as e:\n","            print(f\"Error saving single object to {file_path}: {e}\")\n","            return False\n","\n","    @staticmethod\n","    def load_json_array(file_path, default=None):\n","        \"\"\"\n","        Load JSON array from file.\n","\n","        Args:\n","            file_path (str): Path to the JSON file\n","            default: Default value if file doesn't exist or is invalid\n","\n","        Returns:\n","            list: The loaded array or default value\n","        \"\"\"\n","        if default is None:\n","            default = []\n","\n","        try:\n","            if not os.path.exists(file_path):\n","                return default\n","\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Ensure it's a list\n","            if isinstance(data, list):\n","                return data\n","            else:\n","                return [data]\n","\n","        except (json.JSONDecodeError, FileNotFoundError, Exception) as e:\n","            print(f\"Error loading JSON array from {file_path}: {e}\")\n","            return default\n","\n","    @staticmethod\n","    def load_single_object(file_path, default=None):\n","        \"\"\"\n","        Load a single object from a JSON file.\n","\n","        Args:\n","            file_path (str): Path to the JSON file\n","            default: Default value if file doesn't exist or is invalid\n","\n","        Returns:\n","            Any: The loaded object or default value\n","        \"\"\"\n","        try:\n","            if not os.path.exists(file_path):\n","                return default\n","\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            return data\n","\n","        except (json.JSONDecodeError, FileNotFoundError, Exception) as e:\n","            print(f\"Error loading single object from {file_path}: {e}\")\n","            return default\n","\n","    @staticmethod\n","    def clear_json_array(file_path):\n","        \"\"\"\n","        Clear the JSON array file (set to empty array).\n","\n","        Args:\n","            file_path (str): Path to the JSON file\n","\n","        Returns:\n","            bool: True if successful, False otherwise\n","        \"\"\"\n","        return JSONArrayManager.save_json_array(file_path, [], append=False)\n","\n","    @staticmethod\n","    def get_array_length(file_path):\n","        \"\"\"\n","        Get the length of the JSON array.\n","\n","        Args:\n","            file_path (str): Path to the JSON file\n","\n","        Returns:\n","            int: Length of the array, 0 if file doesn't exist or is invalid\n","        \"\"\"\n","        data = JSONArrayManager.load_json_array(file_path)\n","        return len(data)\n","\n","    @staticmethod\n","    def append_single_object(file_path, obj, create_dirs=True):\n","        \"\"\"\n","        Append a single JSON object to an existing JSON array file.\n","\n","        Args:\n","            file_path (str): Path to the JSON file\n","            obj: Single JSON object to append\n","            create_dirs (bool): If True, create directories if they don't exist\n","\n","        Returns:\n","            bool: True if successful, False otherwise\n","        \"\"\"\n","        try:\n","            # Create directories if needed\n","            if create_dirs:\n","                os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","\n","            # Check if file exists\n","            if not os.path.exists(file_path):\n","                # File doesn't exist - create new file with single object in array\n","                with open(file_path, 'w', encoding='utf-8') as f:\n","                    json.dump([obj], f, indent=2, ensure_ascii=False)\n","                return True\n","\n","            # File exists - read existing data and append\n","            try:\n","                with open(file_path, 'r', encoding='utf-8') as f:\n","                    existing_data = json.load(f)\n","\n","                # Ensure existing data is a list\n","                if not isinstance(existing_data, list):\n","                    existing_data = [existing_data]\n","\n","            except (json.JSONDecodeError, FileNotFoundError):\n","                # File exists but invalid JSON, start with new array\n","                existing_data = []\n","\n","            # Append the single object\n","            existing_data.append(obj)\n","\n","            # Write back to file\n","            with open(file_path, 'w', encoding='utf-8') as f:\n","                json.dump(existing_data, f, indent=2, ensure_ascii=False)\n","\n","            return True\n","\n","        except Exception as e:\n","            print(f\"Error appending single object to {file_path}: {e}\")\n","            return False\n","\n","    @staticmethod\n","    def file_exists_and_not_empty(file_path):\n","        \"\"\"\n","        Check if file exists and contains data.\n","\n","        Args:\n","            file_path (str): Path to the JSON file\n","\n","        Returns:\n","            bool: True if file exists and has data, False otherwise\n","        \"\"\"\n","        return JSONArrayManager.get_array_length(file_path) > 0\n","\n","\n","class MultiVersionSpecificationDownloader:\n","    def __init__(self, mode='full', custom_section=None):\n","        self.mode = mode\n","        self.custom_section = custom_section\n","\n","        # Full specification sections\n","        full_spec_sections = {\n","            \"method_resolution\": {\n","                \"description\": \"Method Resolution and Invocation\",\n","                \"section\": \"15.12\"\n","            },\n","            \"switch_statement\": {\n","                \"description\": \"The switch Statement\",\n","                \"section\": \"14.11\"\n","            },\n","            \"inheritance\": {\n","                \"description\": \"Inheritance and Method Overriding\",\n","                \"section\": \"8.4.8\"\n","            },\n","            \"overloading\": {\n","                \"description\": \"Method Overloading Resolution\",\n","                \"section\": \"15.12.2\"\n","            },\n","            \"generics\": {\n","                \"description\": \"Generic Types and Type Parameters\",\n","                \"section\": \"4.5\"\n","            },\n","            \"exceptions\": {\n","                \"description\": \"Exception Handling\",\n","                \"section\": \"11\"\n","            },\n","            \"interfaces\": {\n","                \"description\": \"Interface Declarations\",\n","                \"section\": \"9\"\n","            },\n","            \"abstract_methods\": {\n","                \"description\": \"Abstract Method Declarations\",\n","                \"section\": \"8.4.3\"\n","            },\n","            \"constructors\": {\n","                \"description\": \"Constructor Declarations\",\n","                \"section\": \"8.8\"\n","            },\n","            \"expressions\": {\n","                \"description\": \"Expressions\",\n","                \"section\": \"15.10.2\"\n","            },\n","            \"threads_and_locks\": {\n","                \"description\": \"Threads and Locks\",\n","                \"section\": \"17\"\n","            },\n","            \"type_inference\": {\n","                \"description\": \"Type Inference\",\n","                \"section\": \"18\"\n","            }\n","        }\n","\n","        # Test mode: Default sections or custom section\n","        if mode == 'test':\n","            if custom_section:\n","                # Use custom section provided by user\n","                self.spec_sections = {\n","                    custom_section['key']: {\n","                        \"description\": custom_section['title'],\n","                        \"section\": custom_section['section_number']\n","                    }\n","                }\n","                print(f\"ðŸŽ¯ Test mode: Analyzing custom section '{custom_section['title']}' (Section {custom_section['section_number']})\")\n","            else:\n","                # Use default test section\n","                test_spec_sections = {\n","                    \"switch_statement\": {\n","                        \"description\": \"The switch Statement\",\n","                        \"section\": \"14.11\"\n","                    }\n","                }\n","                self.spec_sections = test_spec_sections\n","                print(f\"ðŸ§ª Test mode: Using default test section\")\n","        else:\n","            # Full mode: All sections\n","            self.spec_sections = full_spec_sections\n","\n","        # Java versions based on mode\n","        if mode == 'test':\n","            # Test mode: Only 2 versions\n","            self.java_versions = {\n","                \"8\": \"se8\",\n","                \"24\": \"se24\"\n","            }\n","            print(f\"ðŸ“Š Processing {len(self.spec_sections)} section(s) across 2 Java versions\")\n","        else:\n","            # Full mode: All versions\n","            self.java_versions = {\n","                \"8\": \"se8\",\n","                \"11\": \"se11\",\n","                \"17\": \"se17\",\n","                \"21\": \"se21\",\n","                \"24\": \"se24\"\n","            }\n","            print(f\"ðŸš€ Full mode: Processing {len(self.spec_sections)} sections across {len(self.java_versions)} Java versions\")\n","\n","        self.session = requests.Session()\n","        self.session.headers.update({\n","            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n","        })\n","\n","    def build_url(self, java_version: str, section: str) -> str:\n","        \"\"\"Build JLS URL for specific version and section\"\"\"\n","        version_code = self.java_versions.get(java_version, \"se17\")\n","        base_url = f\"https://docs.oracle.com/javase/specs/jls/{version_code}/html\"\n","\n","        # Convert section number to JLS format\n","        if '.' in section:\n","            major, minor = section.split('.', 1)\n","            return f\"{base_url}/jls-{major}.html#jls-{section}\"\n","        else:\n","            return f\"{base_url}/jls-{section}.html\"\n","\n","    def escape_css_selector(self, selector: str) -> str:\n","        \"\"\"Escape dots in CSS selectors for BeautifulSoup\"\"\"\n","        # Replace dots with escaped dots for CSS selectors\n","        return selector.replace('.', '\\\\.')\n","\n","    def extract_section_info(self, soup: BeautifulSoup, java_version: str, section: str) -> Dict[str, str]:\n","        \"\"\"Extract section title and number from HTML\"\"\"\n","        section_info = {\n","            'java_version': java_version,\n","            'section_number': section,\n","            'section_title': 'Unknown',\n","            'chapter_title': 'Unknown'\n","        }\n","\n","        try:\n","            # Escape the section number for CSS selectors\n","            escaped_section = self.escape_css_selector(section)\n","\n","            # Try to find section title with properly escaped selectors\n","            title_selectors = [\n","                f'h2[id=\"jls-{section}\"]',  # Use attribute selector instead\n","                f'h3[id=\"jls-{section}\"]',\n","                f'h1[id=\"jls-{section}\"]',\n","                f'*[id=\"jls-{section}\"]',   # Any element with the ID\n","                '.section-title',\n","                '.chapter-title h1',\n","                'h1', 'h2'\n","            ]\n","\n","            for selector in title_selectors:\n","                try:\n","                    title_elem = soup.select_one(selector)\n","                    if title_elem:\n","                        title_text = title_elem.get_text(strip=True)\n","                        if section in title_text or any(word in title_text.lower() for word in ['method', 'class', 'interface', 'type']):\n","                            section_info['section_title'] = title_text\n","                            break\n","                except Exception as selector_error:\n","                    # Skip invalid selectors and continue\n","                    continue\n","\n","            # Try to find chapter title\n","            try:\n","                chapter_elem = soup.select_one('h1')\n","                if chapter_elem:\n","                    section_info['chapter_title'] = chapter_elem.get_text(strip=True)\n","            except Exception:\n","                pass\n","\n","        except Exception as e:\n","            print(f\"Warning: Could not extract section info: {e}\")\n","\n","        return section_info\n","\n","    def download_section(self, java_version: str, section_name: str, section_number: str) -> Optional[Dict[str, Any]]:\n","        \"\"\"Download and extract text from JLS section for specific Java version\"\"\"\n","        url = self.build_url(java_version, section_number)\n","\n","        try:\n","            print(f\"   ðŸ“¥ Downloading Java {java_version} - {section_name} from {url}\")\n","\n","            response = self.session.get(url, timeout=30)\n","            response.raise_for_status()\n","\n","            soup = BeautifulSoup(response.content, 'html.parser')\n","\n","            # Remove script and style elements\n","            for element in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\"]):\n","                element.decompose()\n","\n","            # Extract section metadata\n","            section_info = self.extract_section_info(soup, java_version, section_number)\n","\n","            # Extract main content using attribute selectors instead of ID selectors with dots\n","            content_selectors = [\n","                f'div[id=\"jls-{section_number}\"]',  # Use attribute selector\n","                f'*[id=\"jls-{section_number}\"]',    # Any element with the ID\n","                'div.section',\n","                'div.chapter',\n","                'main',\n","                'div.content',\n","                'article',\n","                'div[role=\"main\"]',\n","                'body'\n","            ]\n","\n","            content = None\n","            for selector in content_selectors:\n","                try:\n","                    content = soup.select_one(selector)\n","                    if content and content.get_text(strip=True):\n","                        break\n","                except Exception:\n","                    # Skip invalid selectors\n","                    continue\n","\n","            if not content:\n","                # Fallback to the entire soup\n","                content = soup\n","\n","            text = content.get_text(separator=' ', strip=True)\n","\n","            # Clean up the text\n","            text = re.sub(r'\\s+', ' ', text)\n","            text = re.sub(r'\\n+', '\\n', text)\n","\n","            # Remove common navigation text\n","            text = re.sub(r'(Contents|Previous|Next|Index)(\\s+\\||\\s+)?', '', text)\n","            text = re.sub(r'Oracle and/or its affiliates.*?All rights reserved\\.', '', text)\n","\n","            if len(text) < 100:\n","                print(f\"   âš ï¸ Very short content ({len(text)} chars), might be an error\")\n","                return None\n","\n","            result = {\n","                'text': text,\n","                'metadata': section_info,\n","                'url': url,\n","                'section_name': section_name,\n","                'downloaded_at': datetime.now().isoformat()\n","            }\n","\n","            print(f\"   âœ… Downloaded {len(text)} characters\")\n","            return result\n","\n","        except requests.exceptions.RequestException as e:\n","            print(f\"   âŒ Network error downloading {url}: {e}\")\n","            return None\n","        except Exception as e:\n","            print(f\"   âŒ Error processing {url}: {e}\")\n","            return None\n","\n","    def download_all_sections(self) -> Dict[str, Dict[str, Any]]:\n","        \"\"\"Download all JLS sections for selected Java versions\"\"\"\n","        all_sections = {}\n","\n","        print(f\"ðŸ“š Downloading {len(self.spec_sections)} section(s) across {len(self.java_versions)} Java versions...\")\n","\n","        for section_name, section_config in self.spec_sections.items():\n","            section_number = section_config['section']\n","            description = section_config['description']\n","\n","            print(f\"\\nðŸ“– Processing: {description}\")\n","\n","            for java_version in self.java_versions.keys():\n","                key = f\"{section_name}_java{java_version}\"\n","\n","                section_data = self.download_section(java_version, section_name, section_number)\n","\n","                if section_data:\n","                    all_sections[key] = section_data\n","\n","                    # Save individual section to file\n","                    file_path = f'{project_path}/data/{key}.json'\n","                    JSONArrayManager.save_single_object(file_path, section_data)\n","                else:\n","                    print(f\"   âš ï¸ Failed to download Java {java_version} {section_name}\")\n","\n","                # Longer delay in test mode to be respectful with free models\n","                delay = 2 if self.mode == 'test' else 1\n","                time.sleep(delay)\n","\n","        print(f\"\\nâœ… Downloaded {len(all_sections)} specification sections total\")\n","        return all_sections\n","\n","# =============================================================================\n","# PHASE 4: RULE EXTRACTION WITH VERSION TRACKING\n","# =============================================================================\n","\n","class EnhancedSpecPreprocessor:\n","    def __init__(self):\n","        self.nlp = spacy.load('en_core_web_sm')\n","\n","        # rule patterns with more comprehensive detection\n","        self.rule_patterns = [\n","            r'\\b(must|shall|should|will|may|cannot|must not|may not|required to|prohibited from)\\b',\n","            r'\\b(if.*then|when.*then|unless|provided that|except when)\\b',\n","            r'\\b(required|mandatory|optional|forbidden|prohibited|allowed|permitted)\\b',\n","            r'\\b(always|never|only when|only if|if and only if|whenever)\\b',\n","            r'\\b(compile.time error|runtime error|exception|compilation error)\\b',\n","            r'\\b(every|all|any|some|no|none)\\b.*\\b(method|class|interface|type|variable)\\b',\n","            r'\\b(override|overload|implement|extend|inherit)\\b',\n","            r'\\b(accessible|visible|private|protected|public|package.private)\\b'\n","        ]\n","\n","    def extract_sentences(self, text: str) -> List[str]:\n","        \"\"\"Extract sentences from specification text with better filtering\"\"\"\n","        # Split into sentences\n","        sentences = sent_tokenize(text)\n","\n","        # Filter and clean sentences\n","        cleaned_sentences = []\n","        for sent in sentences:\n","            # Clean whitespace\n","            sent = re.sub(r'\\s+', ' ', sent).strip()\n","\n","            # Skip very short sentences, page numbers, navigation text\n","            if len(sent) < 15:\n","                continue\n","\n","            # Skip sentences that are likely navigation or formatting\n","            skip_patterns = [\n","                r'^\\d+(\\.\\d+)*$',  # Just numbers\n","                r'^(Contents|Previous|Next|Index|Chapter|Section)',\n","                r'^Oracle and/or',\n","                r'^Copyright',\n","                r'^All rights reserved'\n","            ]\n","\n","            if any(re.match(pattern, sent) for pattern in skip_patterns):\n","                continue\n","\n","            # Filter reasonable length sentences\n","            if 15 <= len(sent) <= 800:\n","                cleaned_sentences.append(sent)\n","\n","        return cleaned_sentences\n","\n","    def is_rule_sentence(self, sentence: str) -> bool:\n","        \"\"\"Rule detection with better pattern matching\"\"\"\n","        # Check for rule indicators\n","        has_rule_pattern = any(re.search(pattern, sentence, re.IGNORECASE) for pattern in self.rule_patterns)\n","\n","        # Additional checks for specification language\n","        spec_indicators = [\n","            r'\\b(specification|standard|requirement)\\b',\n","            r'\\b(behavior|behaviour|semantics)\\b',\n","            r'\\b(valid|invalid|legal|illegal)\\b',\n","            r'\\b(throws?|catch|exception handling)\\b'\n","        ]\n","\n","        has_spec_language = any(re.search(pattern, sentence, re.IGNORECASE) for pattern in spec_indicators)\n","\n","        # Avoid sentences that are just examples or notes\n","        avoid_patterns = [\n","            r'^(For example|Note that|See also|Example)',\n","            r'^(The following|As shown|Consider)',\n","            r'^\\d+\\.\\d+',  # Section numbers\n","        ]\n","\n","        has_avoid_pattern = any(re.match(pattern, sentence, re.IGNORECASE) for pattern in avoid_patterns)\n","\n","        return (has_rule_pattern or has_spec_language) and not has_avoid_pattern\n","\n","    def extract_rules(self, section_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n","        \"\"\"Extract rule-like sentences with version and section metadata\"\"\"\n","        text = section_data.get('text', '')\n","        metadata = section_data.get('metadata', {})\n","\n","        sentences = self.extract_sentences(text)\n","        rules = []\n","\n","        for sent in sentences:\n","            if self.is_rule_sentence(sent):\n","                # Process with spaCy\n","                doc = self.nlp(sent)\n","\n","                # Extract linguistic features\n","                entities = [(ent.text, ent.label_) for ent in doc.ents]\n","                dependencies = [(token.text, token.dep_, token.head.text) for token in doc]\n","\n","                # Identify modal verbs and their context\n","                modals = []\n","                for token in doc:\n","                    if token.text.lower() in ['must', 'shall', 'should', 'may', 'cannot', 'will']:\n","                        context_start = max(0, token.i - 3)\n","                        context_end = min(len(doc), token.i + 4)\n","                        context = ' '.join([t.text for t in doc[context_start:context_end]])\n","\n","                        modals.append({\n","                            'modal': token.text.lower(),\n","                            'context': context\n","                        })\n","\n","                # Create rule with metadata\n","                rule = {\n","                    'text': sent,\n","                    'java_version': metadata.get('java_version', 'unknown'),\n","                    'section_number': metadata.get('section_number', 'unknown'),\n","                    'section_title': metadata.get('section_title', 'unknown'),\n","                    'chapter_title': metadata.get('chapter_title', 'unknown'),\n","                    'section_name': section_data.get('section_name', 'unknown'),\n","                    'source_url': section_data.get('url', ''),\n","                    'extracted_at': datetime.now().isoformat(),\n","                    'entities': entities,\n","                    'dependencies': dependencies,\n","                    'modals': modals,\n","                    'tokens': [token.text for token in doc],\n","                    'pos_tags': [(token.text, token.pos_) for token in doc]\n","                }\n","\n","                rules.append(rule)\n","\n","        return rules\n","\n","# =============================================================================\n","# PHASE 5: LLM-BASED RULE ANALYSIS WITH BETTER PROMPTS\n","# =============================================================================\n","\n","class EnhancedLLMRuleAnalyzer:\n","    def __init__(self, llm_client):\n","        self.llm_client = llm_client\n","        self.categories = [\n","            \"METHOD_RESOLUTION\",\n","            \"TYPE_COMPATIBILITY\",\n","            \"INHERITANCE_RULES\",\n","            \"OVERLOADING_RULES\",\n","            \"ACCESS_CONTROL\",\n","            \"EXCEPTION_HANDLING\",\n","            \"GENERICS_RULES\",\n","            \"INTERFACE_RULES\",\n","            \"CONSTRUCTOR_RULES\",\n","            \"COMPILATION_RULES\"\n","        ]\n","\n","    def categorize_rule(self, rule_text: str, java_version: str, section_info: str) -> str:\n","        \"\"\"Rule categorization with better context\"\"\"\n","        prompt = f\"\"\"You are analyzing Java Language Specification rules. Categorize this rule into exactly ONE category.\n","\n","Java Version: {java_version}\n","Section Context: {section_info}\n","\n","Rule Text: \"{rule_text}\"\n","\n","Available Categories:\n","- METHOD_RESOLUTION: Rules about how method calls are resolved\n","- TYPE_COMPATIBILITY: Rules about type assignments and conversions\n","- INHERITANCE_RULES: Rules about class inheritance and method overriding\n","- OVERLOADING_RULES: Rules about method overloading resolution\n","- ACCESS_CONTROL: Rules about visibility and access modifiers\n","- EXCEPTION_HANDLING: Rules about throwing, catching, and declaring exceptions\n","- GENERICS_RULES: Rules about generic types and type parameters\n","- INTERFACE_RULES: Rules about interface declarations and implementations\n","- CONSTRUCTOR_RULES: Rules about constructor declarations and invocation\n","- COMPILATION_RULES: Rules about compile-time checking and errors\n","\n","Choose the MOST SPECIFIC category that applies. Return only the category name.\"\"\"\n","\n","        try:\n","            response = self.llm_client.call_llm(prompt, max_tokens=50, temperature=0.1)\n","            category = response.strip().upper().replace(' ', '_')\n","            return category if category in self.categories else \"COMPILATION_RULES\"\n","        except Exception as e:\n","            print(f\"Error categorizing rule: {e}\")\n","            return \"COMPILATION_RULES\"\n","\n","    def extract_rule_components(self, rule_text: str, java_version: str) -> Dict[str, Any]:\n","        \"\"\"Component extraction with structured analysis\"\"\"\n","        prompt = f\"\"\"Analyze this Java {java_version} specification rule and extract its key components.\n","\n","Rule: \"{rule_text}\"\n","\n","Extract the following components and return ONLY valid JSON:\n","\n","{{\n","    \"subject\": \"What this rule applies to (e.g., 'method call', 'class declaration', 'type parameter')\",\n","    \"action\": \"The main action/relationship (e.g., 'overrides', 'implements', 'resolves to', 'throws')\",\n","    \"modality\": \"Requirement strength (must/shall/should/may/cannot/prohibited)\",\n","    \"conditions\": [\"List of conditions that must be met\", \"for this rule to apply\"],\n","    \"consequences\": [\"What happens when rule applies\", \"expected outcomes or requirements\"],\n","    \"exceptions\": [\"Any stated exceptions to this rule\", \"special cases where rule doesn't apply\"],\n","    \"scope\": \"When this rule applies (compile-time/runtime/both)\"\n","}}\n","\n","Focus on extracting concrete, specific information. If a field is not clearly present, use an empty array [] or \"not specified\".\"\"\"\n","\n","        try:\n","            response = self.llm_client.call_llm(prompt, max_tokens=600, temperature=0.2)\n","            # Extract JSON from response\n","            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n","            if json_match:\n","                return json.loads(json_match.group())\n","            return {\n","                \"subject\": \"not specified\",\n","                \"action\": \"not specified\",\n","                \"modality\": \"not specified\",\n","                \"conditions\": [],\n","                \"consequences\": [],\n","                \"exceptions\": [],\n","                \"scope\": \"not specified\"\n","            }\n","        except Exception as e:\n","            print(f\"Error extracting components: {e}\")\n","            return {}\n","\n","    def convert_to_formal_logic(self, rule_text: str, components: Dict, java_version: str) -> str:\n","        \"\"\"Formal logic conversion with Java-specific predicates\"\"\"\n","        prompt = f\"\"\"Convert this Java {java_version} specification rule to formal first-order logic.\n","\n","Rule: \"{rule_text}\"\n","Components: {json.dumps(components, indent=2)}\n","\n","Use these Java-specific predicates:\n","- Method(m), Class(c), Interface(i), Type(t), Variable(v)\n","- Overrides(m1,m2), Implements(c,i), Extends(c1,c2)\n","- Accessible(x,context), Compatible(t1,t2), Assignable(t1,t2)\n","- Throws(m,exception), Declares(entity,property)\n","- CompileTime(condition), Runtime(condition)\n","- HasModifier(entity,modifier) where modifier âˆˆ {{public,private,protected,static,final,abstract}}\n","\n","Logical operators: âˆ€ (forall), âˆƒ (exists), â†’ (implies), âˆ§ (and), âˆ¨ (or), Â¬ (not)\n","\n","Express the rule as a clear logical statement. If the rule has conditions, use implications (â†’).\n","\n","Example format:\n","âˆ€m,c: Method(m) âˆ§ Declares(c,m) âˆ§ HasModifier(m,private) â†’ Â¬Accessible(m,Subclass(c))\n","\n","Return only the formal logic expression:\"\"\"\n","\n","        try:\n","            response = self.llm_client.call_llm(prompt, max_tokens=300, temperature=0.1)\n","            return response.strip()\n","        except Exception as e:\n","            print(f\"Error converting to formal logic: {e}\")\n","            return \"Logic conversion failed\"\n","\n","# =============================================================================\n","# PHASE 6: MODE-AWARE CONFLICT DETECTION ENGINE\n","# =============================================================================\n","\n","class EnhancedConflictDetector:\n","    def __init__(self, llm_client, mode='full', project_path = None, max_llm_calls=1000):\n","        self.llm_client = llm_client\n","        self.mode = mode\n","        self.max_llm_calls = max_llm_calls\n","        self.project_path = project_path\n","        self.llm_calls_made = 0\n","        self.rules_db = []\n","        self.potential_conflicts = []\n","        self.conflict_types = [\n","            \"CONTRADICTION\",  # Rules that directly contradict each other\n","            \"AMBIGUITY\",      # Rules that create unclear situations\n","            \"OVERLAP\",        # Rules that apply to same situation differently\n","            \"VERSION_CHANGE\", # Rules that changed between Java versions\n","            \"SCOPE_CONFLICT\", # Rules with overlapping but different scopes\n","            \"PRECEDENCE\"      # Rules with unclear precedence order\n","        ]\n","\n","    def add_rule(self, rule_data: Dict[str, Any]):\n","        \"\"\"Add processed rule to database with indexing\"\"\"\n","        rule_id = f\"rule_{len(self.rules_db)}_{rule_data.get('java_version', 'unknown')}\"\n","\n","        enhanced_rule = {\n","            'id': rule_id,\n","            'text': rule_data.get('text', ''),\n","            'java_version': rule_data.get('java_version', 'unknown'),\n","            'section_number': rule_data.get('section_number', 'unknown'),\n","            'section_title': rule_data.get('section_title', 'unknown'),\n","            'chapter_title': rule_data.get('chapter_title', 'unknown'),\n","            'section_name': rule_data.get('section_name', 'unknown'),\n","            'source_url': rule_data.get('source_url', ''),\n","            'extracted_at': rule_data.get('extracted_at', datetime.now().isoformat()),\n","            'processed_at': rule_data.get('processed_at', datetime.now().isoformat()),\n","            'entities': rule_data.get('entities', []),\n","            'dependencies': rule_data.get('dependencies', []),\n","            'modals': rule_data.get('modals', []),\n","            'tokens': rule_data.get('tokens', []),\n","            'pos_tags': rule_data.get('pos_tags', []),\n","            'category': rule_data.get('category', 'UNKNOWN'),\n","            'components': rule_data.get('components', {}),\n","            'formal_logic': rule_data.get('formal_logic', ''),\n","            'keywords': self._extract_keywords(rule_data.get('text', '')),\n","            'modality': self._extract_modality_from_components(rule_data.get('components', {})),\n","            'scope': self._extract_scope_from_components(rule_data.get('components', {})),\n","            'added_at': datetime.now().isoformat()\n","        }\n","\n","        self.rules_db.append(enhanced_rule)\n","        return rule_id\n","\n","    def _extract_modality_from_components(self, components: Dict[str, Any]) -> str:\n","        \"\"\"Extract modality information from components dictionary\"\"\"\n","        if not components:\n","            return 'not specified'\n","\n","        # Check if components has modality information\n","        if 'modality' in components:\n","            return components['modality']\n","\n","        # If no explicit modality, try to infer from other component fields\n","        # This depends on what's actually in your components structure\n","        return 'not specified'\n","\n","    def _extract_scope_from_components(self, components: Dict[str, Any]) -> str:\n","        \"\"\"Extract scope information from components dictionary\"\"\"\n","        if not components:\n","            return 'not specified'\n","\n","        # Check if components has scope information\n","        if 'scope' in components:\n","            return components['scope']\n","\n","        # If no explicit scope, try to infer from other component fields\n","        # This depends on what's actually in your components structure\n","        return 'not specified'\n","\n","    def _extract_keywords(self, text: str) -> List[str]:\n","        \"\"\"Extract key terms from rule text for matching\"\"\"\n","        # Common Java language specification keywords\n","        java_keywords = [\n","            'method', 'class', 'interface', 'type', 'variable', 'field',\n","            'constructor', 'inheritance', 'override', 'overload', 'implement',\n","            'extend', 'abstract', 'final', 'static', 'private', 'protected',\n","            'public', 'package', 'generic', 'parameter', 'argument', 'return',\n","            'exception', 'throw', 'catch', 'compile', 'runtime', 'accessible',\n","            'visible', 'compatible', 'assignable', 'resolution', 'invocation'\n","        ]\n","\n","        text_lower = text.lower()\n","        found_keywords = []\n","\n","        for keyword in java_keywords:\n","            if keyword in text_lower:\n","                found_keywords.append(keyword)\n","\n","        # Also extract quoted terms and technical terms\n","        quoted_terms = re.findall(r'\"([^\"]*)\"', text)\n","        technical_terms = re.findall(r'\\b[A-Z][a-zA-Z]*(?:[A-Z][a-zA-Z]*)*\\b', text)\n","\n","        found_keywords.extend(quoted_terms)\n","        found_keywords.extend(technical_terms)\n","\n","        return list(set(found_keywords))  # Remove duplicates\n","\n","    def find_potential_conflicts(self) -> List[Dict[str, Any]]:\n","        \"\"\"Find potential conflicts between rules with mode-aware analysis\"\"\"\n","        conflicts = []\n","\n","        # In test mode, limit comparisons to reduce processing time\n","        if self.mode == 'test':\n","            max_comparisons = 50\n","            print(f\"ðŸ§ª Test mode: Limiting to {max_comparisons} rule comparisons\")\n","        else:\n","            max_comparisons = len(self.rules_db) * (len(self.rules_db) - 1) // 2\n","            print(f\"ðŸš€ Full mode: Analyzing {max_comparisons} rule combinations\")\n","\n","        comparisons_made = 0\n","\n","        for i, rule1 in enumerate(self.rules_db):\n","            for j, rule2 in enumerate(self.rules_db[i+1:], i+1):\n","                if comparisons_made >= max_comparisons:\n","                    break\n","\n","                # Quick pre-filtering\n","                if self._should_compare_rules(rule1, rule2):\n","                    conflict = self._analyze_rule_pair(rule1, rule2)\n","                    if conflict:\n","                        conflicts.append(conflict)\n","                        print(f\"   ðŸ” Potential conflict found: {conflict['type']} between Java {rule1['java_version']} and {rule2['java_version']}\")\n","                        file_path = f'{self.project_path}/results/all_conflicts.json'\n","                        JSONArrayManager.append_single_object(file_path, conflict)\n","\n","                comparisons_made += 1\n","\n","                # Progress indicator for full mode\n","                if self.mode == 'full' and comparisons_made % 100 == 0:\n","                    print(f\"   ðŸ“Š Analyzed {comparisons_made}/{max_comparisons} combinations...\")\n","\n","            if comparisons_made >= max_comparisons:\n","                break\n","\n","        self.potential_conflicts = conflicts\n","        print(f\"âœ… Found {len(conflicts)} potential conflicts\")\n","        return conflicts\n","\n","    def _should_compare_rules(self, rule1: Dict, rule2: Dict) -> bool:\n","        \"\"\"Quick filtering to determine if two rules should be compared\"\"\"\n","\n","        # Don't compare identical rules\n","        if rule1.get('id') == rule2.get('id'):\n","            return False\n","\n","        def make_hashable_set(items):\n","            \"\"\"Convert a list of potentially unhashable items to a set of hashable items\"\"\"\n","            if not items:\n","                return set()\n","\n","            hashable_items = []\n","            for item in items:\n","                if isinstance(item, list):\n","                    # Convert list to tuple\n","                    hashable_items.append(tuple(item))\n","                elif isinstance(item, dict):\n","                    # Convert dict to tuple of sorted items\n","                    hashable_items.append(tuple(sorted(item.items())))\n","                elif isinstance(item, (str, int, float, bool, type(None))):\n","                    # Already hashable\n","                    hashable_items.append(item)\n","                else:\n","                    # Try to convert to string as fallback\n","                    hashable_items.append(str(item))\n","\n","            return set(hashable_items)\n","\n","        try:\n","            # Compare rules with overlapping entities\n","            entities1 = make_hashable_set(rule1.get('entities', []))\n","            entities2 = make_hashable_set(rule2.get('entities', []))\n","            common_entities = entities1 & entities2\n","\n","            # Compare rules with overlapping tokens\n","            tokens1 = make_hashable_set(rule1.get('tokens', []))\n","            tokens2 = make_hashable_set(rule2.get('tokens', []))\n","            common_tokens = tokens1 & tokens2\n","\n","            # Compare rules with overlapping modals\n","            modals1 = make_hashable_set(rule1.get('modals', []))\n","            modals2 = make_hashable_set(rule2.get('modals', []))\n","            common_modals = modals1 & modals2\n","\n","        except Exception as e:\n","            print(f\"âš ï¸ Warning: Error processing rule comparison data: {e}\")\n","            # If we can't process the data safely, err on the side of comparison\n","            return True\n","\n","        # Don't compare if they have no overlapping content\n","        if len(common_entities) == 0 and len(common_tokens) == 0 and len(common_modals) == 0:\n","            return False\n","\n","        # Always compare rules from different Java versions with same category\n","        if (rule1.get('java_version') != rule2.get('java_version') and\n","            rule1.get('category') == rule2.get('category')):\n","            return True\n","\n","        # Compare rules with similar scope or modality\n","        if (rule1.get('scope') == rule2.get('scope') or\n","            rule1.get('modality') == rule2.get('modality')):\n","            return True\n","\n","        # Compare rules with overlapping entities (already calculated above)\n","        if len(common_entities) > 0:\n","            return True\n","\n","        # Compare rules with overlapping modals\n","        if len(common_modals) > 0:\n","            return True\n","\n","        # Compare rules with significant token overlap\n","        if len(common_tokens) > 0:\n","            total_unique_tokens = len(tokens1 | tokens2)\n","            overlap_ratio = len(common_tokens) / max(total_unique_tokens, 1)\n","            return overlap_ratio > 0.1  # Lower threshold since tokens are more granular\n","\n","        return False\n","\n","    def _safe_list_to_string(self, items: List[Any], max_items: int = 5) -> str:\n","        \"\"\"Safely convert a list of mixed types to a string representation\"\"\"\n","        if not items:\n","            return \"None\"\n","\n","        # Take only the first max_items\n","        limited_items = items[:max_items]\n","        string_items = []\n","\n","        for item in limited_items:\n","            if isinstance(item, dict):\n","                # Convert dict to a brief string representation\n","                if len(item) == 1:\n","                    key, value = next(iter(item.items()))\n","                    string_items.append(f\"{key}:{value}\")\n","                else:\n","                    string_items.append(f\"dict({len(item)} keys)\")\n","            elif isinstance(item, list):\n","                string_items.append(f\"list({len(item)} items)\")\n","            elif isinstance(item, str):\n","                # Truncate long strings\n","                if len(item) > 50:\n","                    string_items.append(f\"{item[:47]}...\")\n","                else:\n","                    string_items.append(item)\n","            else:\n","                string_items.append(str(item))\n","\n","        result = \", \".join(string_items)\n","\n","        # Add indication if there are more items\n","        if len(items) > max_items:\n","            result += f\" (and {len(items) - max_items} more)\"\n","\n","        return result\n","\n","    def _analyze_rule_pair(self, rule1: Dict, rule2: Dict) -> Optional[Dict[str, Any]]:\n","        \"\"\"Analyze a pair of rules for potential conflicts using LLM\"\"\"\n","\n","        def safe_common_entities(entities1, entities2):\n","            \"\"\"Safely find common entities between two lists\"\"\"\n","            if not entities1 or not entities2:\n","                return []\n","\n","            # Convert to hashable format for comparison\n","            def make_hashable(items):\n","                hashable_items = []\n","                for item in items:\n","                    if isinstance(item, list):\n","                        hashable_items.append(tuple(item))\n","                    elif isinstance(item, dict):\n","                        hashable_items.append(tuple(sorted(item.items())))\n","                    else:\n","                        hashable_items.append(item)\n","                return set(hashable_items)\n","\n","            try:\n","                set1 = make_hashable(entities1)\n","                set2 = make_hashable(entities2)\n","                common_hashable = set1 & set2\n","\n","                # Convert back to list of strings for JSON serialization\n","                return [str(item) for item in common_hashable]\n","            except Exception as e:\n","                print(f\"   âš ï¸ Error finding common entities: {e}\")\n","                return []\n","\n","        # Check if we've exceeded the maximum number of LLM calls\n","        if self.llm_calls_made >= self.max_llm_calls:\n","            print(f\"   âš ï¸ LLM call limit exceeded ({self.max_llm_calls}). Skipping further analysis.\")\n","\n","            # Safely process entities\n","            entities1 = rule1.get('entities', [])\n","            entities2 = rule2.get('entities', [])\n","            common_entities = safe_common_entities(entities1, entities2)\n","\n","            return {\n","                'rule1_id': rule1['id'],\n","                'rule2_id': rule2['id'],\n","                'rule1_version': rule1['java_version'],\n","                'rule2_version': rule2['java_version'],\n","                'rule1_section': rule1['section_number'],\n","                'rule2_section': rule2['section_number'],\n","                'rule1_section_title': rule1['section_title'],\n","                'rule2_section_title': rule2['section_title'],\n","                'rule1_chapter': rule1['chapter_title'],\n","                'rule2_chapter': rule2['chapter_title'],\n","                'type': 'CALL_LIMIT_EXCEEDED',\n","                'severity': 'INFO',\n","                'description': f'Analysis skipped due to LLM call limit ({self.max_llm_calls}) being exceeded',\n","                'affected_scenarios': ['Analysis incomplete due to call limit'],\n","                'resolution_needed': 'Increase max_llm_calls parameter or run analysis in batches',\n","                'detected_at': datetime.now().isoformat(),\n","                'rule1_text': rule1['text'][:200] + '...',\n","                'rule2_text': rule2['text'][:200] + '...',\n","                'common_entities': common_entities,\n","                'rule1_url': rule1['source_url'],\n","                'rule2_url': rule2['source_url'],\n","                'call_limit_exceeded': True\n","            }\n","\n","        try:\n","            # Increment the call counter\n","            self.llm_calls_made += 1\n","\n","            # Safely process all list fields that might contain mixed types\n","            entities1_str = self._safe_list_to_string(rule1.get('entities', []))\n","            entities2_str = self._safe_list_to_string(rule2.get('entities', []))\n","            modals1_str = self._safe_list_to_string(rule1.get('modals', []))\n","            modals2_str = self._safe_list_to_string(rule2.get('modals', []))\n","\n","            # Safely handle components - convert to string if it's a dict\n","            components1 = rule1.get('components', {})\n","            components2 = rule2.get('components', {})\n","            components1_str = str(components1)[:100] + (\"...\" if len(str(components1)) > 100 else \"\")\n","            components2_str = str(components2)[:100] + (\"...\" if len(str(components2)) > 100 else \"\")\n","\n","            conflict_analysis_prompt = f\"\"\"Analyze these two Java Language Specification rules for potential conflicts.\n","\n","RULE 1 (Java {rule1['java_version']}, Section {rule1['section_number']} - {rule1['section_title']}):\n","\"{rule1['text']}\"\n","Category: {rule1['category']}\n","Modality: {rule1['modality']}\n","Scope: {rule1['scope']}\n","Entities: {entities1_str}\n","Modals: {modals1_str}\n","Components: {components1_str}\n","\n","RULE 2 (Java {rule2['java_version']}, Section {rule2['section_number']} - {rule2['section_title']}):\n","\"{rule2['text']}\"\n","Category: {rule2['category']}\n","Modality: {rule2['modality']}\n","Scope: {rule2['scope']}\n","Entities: {entities2_str}\n","Modals: {modals2_str}\n","Components: {components2_str}\n","\n","Analyze for these conflict types:\n","1. CONTRADICTION - Rules directly contradict each other\n","2. AMBIGUITY - Rules create unclear or ambiguous situations\n","3. OVERLAP - Rules apply to same situation with different requirements\n","4. VERSION_CHANGE - Rule changed between Java versions\n","5. SCOPE_CONFLICT - Overlapping but different scopes\n","6. PRECEDENCE - Unclear which rule takes precedence\n","\n","Return ONLY this JSON format:\n","{{\n","    \"has_conflict\": true/false,\n","    \"conflict_type\": \"TYPE_FROM_ABOVE_OR_NONE\",\n","    \"severity\": \"LOW/MEDIUM/HIGH\",\n","    \"description\": \"Detailed explanation of the conflict\",\n","    \"affected_scenarios\": [\"Specific scenarios where conflict occurs\"],\n","    \"resolution_needed\": \"What needs clarification\"\n","}}\n","\n","Be precise and only identify genuine conflicts, not minor differences.\"\"\"\n","\n","            print(f\"   ðŸ” LLM Call #{self.llm_calls_made}/{self.max_llm_calls}: Analyzing rule pair...\")\n","\n","            response = self.llm_client.call_llm(\n","                conflict_analysis_prompt,\n","                max_tokens=500,\n","                temperature=0.1\n","            )\n","\n","            # Extract JSON from response\n","            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n","            if json_match:\n","                analysis = json.loads(json_match.group())\n","\n","                if analysis.get('has_conflict', False):\n","                    # Safely calculate common entities\n","                    common_entities = safe_common_entities(\n","                        rule1.get('entities', []),\n","                        rule2.get('entities', [])\n","                    )\n","\n","                    return {\n","                        'rule1_id': rule1['id'],\n","                        'rule2_id': rule2['id'],\n","                        'rule1_version': rule1['java_version'],\n","                        'rule2_version': rule2['java_version'],\n","                        'rule1_section': rule1['section_number'],\n","                        'rule2_section': rule2['section_number'],\n","                        'rule1_section_title': rule1['section_title'],\n","                        'rule2_section_title': rule2['section_title'],\n","                        'rule1_chapter': rule1['chapter_title'],\n","                        'rule2_chapter': rule2['chapter_title'],\n","                        'type': analysis.get('conflict_type', 'UNKNOWN'),\n","                        'severity': analysis.get('severity', 'MEDIUM'),\n","                        'description': analysis.get('description', ''),\n","                        'affected_scenarios': analysis.get('affected_scenarios', []),\n","                        'resolution_needed': analysis.get('resolution_needed', ''),\n","                        'detected_at': datetime.now().isoformat(),\n","                        'rule1_text': rule1['text'][:200] + '...',\n","                        'rule2_text': rule2['text'][:200] + '...',\n","                        'common_entities': common_entities,\n","                        'rule1_url': rule1['source_url'],\n","                        'rule2_url': rule2['source_url'],\n","                        'call_limit_exceeded': False\n","                    }\n","\n","            return None\n","\n","        except Exception as e:\n","            print(f\"   âš ï¸ Error analyzing rule pair: {e}\")\n","            return None\n","\n","    def generate_conflict_report(self) -> Dict[str, Any]:\n","        \"\"\"Generate comprehensive conflict analysis report\"\"\"\n","        if not self.potential_conflicts:\n","            return {\n","                'summary': 'No conflicts detected',\n","                'total_conflicts': 0,\n","                'by_type': {},\n","                'by_severity': {},\n","                'by_version_pair': {},\n","                'by_category': {},\n","                'recommendations': [],\n","                'llm_calls_made': self.llm_calls_made,\n","                'llm_calls_limit': self.max_llm_calls,\n","                'call_limit_reached': self.llm_calls_made >= self.max_llm_calls\n","            }\n","\n","        # Analyze conflicts by type\n","        by_type = {}\n","        by_severity = {}\n","        by_version_pair = {}\n","        by_category = {}\n","\n","        for conflict in self.potential_conflicts:\n","            # By type\n","            conflict_type = conflict.get('type', 'UNKNOWN')\n","            by_type[conflict_type] = by_type.get(conflict_type, 0) + 1\n","\n","            # By severity\n","            severity = conflict.get('severity', 'MEDIUM')\n","            by_severity[severity] = by_severity.get(severity, 0) + 1\n","\n","            # By version pair\n","            v1, v2 = conflict['rule1_version'], conflict['rule2_version']\n","            version_pair = f\"Java {v1} vs Java {v2}\"\n","            by_version_pair[version_pair] = by_version_pair.get(version_pair, 0) + 1\n","\n","            # By category (if we can determine it from the rules)\n","            # This would need access to the original rules to get category info\n","            # For now, we'll skip this or use a simplified approach\n","\n","        # Generate recommendations\n","        recommendations = self._generate_recommendations(by_type, by_severity, by_version_pair)\n","\n","        # Add call limit recommendations if limit was reached\n","        if self.llm_calls_made >= self.max_llm_calls:\n","            recommendations.insert(0,\n","                f\"âš ï¸ LLM call limit ({self.max_llm_calls}) reached. Analysis may be incomplete. Consider increasing max_llm_calls parameter.\"\n","            )\n","\n","        report = {\n","            'total_conflicts': len(self.potential_conflicts),\n","            'by_type': by_type,\n","            'by_severity': by_severity,\n","            'by_version_pair': by_version_pair,\n","            'by_category': by_category,\n","            'recommendations': recommendations,\n","            'detailed_conflicts': self.potential_conflicts[:10],  # Top 10 conflicts\n","            'analysis_mode': self.mode,\n","            'rules_analyzed': len(self.rules_db),\n","            'llm_calls_made': self.llm_calls_made,\n","            'llm_calls_limit': self.max_llm_calls,\n","            'call_limit_reached': self.llm_calls_made >= self.max_llm_calls,\n","            'generated_at': datetime.now().isoformat()\n","        }\n","\n","        return report\n","\n","    def _generate_recommendations(self, by_type: Dict, by_severity: Dict, by_version_pair: Dict) -> List[str]:\n","        \"\"\"Generate actionable recommendations based on conflict analysis\"\"\"\n","        recommendations = []\n","\n","        # High severity recommendations\n","        if by_severity.get('HIGH', 0) > 0:\n","            recommendations.append(\n","                f\"ðŸš¨ URGENT: {by_severity['HIGH']} high-severity conflicts require immediate attention\"\n","            )\n","\n","        # Version-specific recommendations\n","        for version_pair, count in by_version_pair.items():\n","            if count > 3:  # Threshold for version conflict attention\n","                recommendations.append(\n","                    f\"ðŸ“‹ Review {version_pair} compatibility - {count} conflicts detected\"\n","                )\n","\n","        # Type-specific recommendations\n","        if by_type.get('CONTRADICTION', 0) > 0:\n","            recommendations.append(\n","                f\"âš ï¸ {by_type['CONTRADICTION']} direct contradictions need specification clarification\"\n","            )\n","\n","        if by_type.get('VERSION_CHANGE', 0) > 0:\n","            recommendations.append(\n","                f\"ðŸ”„ {by_type['VERSION_CHANGE']} version changes require migration guidance\"\n","            )\n","\n","        # General recommendations\n","        if len(by_type) > 3:\n","            recommendations.append(\n","                \"ðŸ“š Consider creating a unified specification guide to address multiple conflict types\"\n","            )\n","\n","        return recommendations\n","\n","# =============================================================================\n","# PHASE 7: EXECUTION ORCHESTRATOR\n","# =============================================================================\n","\n","class SpecSentinelOrchestrator:\n","    def __init__(self, mode='full', custom_section=None, max_llm_calls=1000):\n","        self.mode = mode\n","        self.custom_section = custom_section\n","        self.max_llm_calls = max_llm_calls\n","        self.project_path = None\n","        self.results = {}\n","\n","    def run_complete_analysis(self):\n","        \"\"\"Run the complete SpecSentinel analysis pipeline\"\"\"\n","        start_time = datetime.now()\n","\n","        try:\n","            # Phase 1: Setup\n","            print(f\"\\n{'='*60}\")\n","            print(f\"ðŸš€ STARTING SPECSENTINEL ANALYSIS ({self.mode.upper()} MODE)\")\n","            print(f\"{'='*60}\")\n","\n","            self.project_path = setup_project_path(self.mode)\n","\n","            # Phase 2: Initialize LLM client\n","            print(f\"\\nðŸ“¡ Phase 2: Initializing LLM Client...\")\n","            llm_client = EnhancedOpenRouterClient(self.mode)\n","\n","            # # Phase 3: Download specifications\n","            # print(f\"\\nðŸ“š Phase 3: Downloading Java Specifications...\")\n","            # downloader = MultiVersionSpecificationDownloader(self.mode, self.custom_section)\n","            # specifications = downloader.download_all_sections()\n","\n","            # if not specifications:\n","            #     raise Exception(\"Failed to download specifications\")\n","\n","            # # Save specifications\n","            file_path = f'{self.project_path}/data/all_specifications.json'\n","            # JSONArrayManager.save_json_array(file_path, specifications)\n","\n","            specifications = JSONArrayManager.load_json_array(file_path)\n","\n","\n","            # # Phase 4: Extract rules\n","            # print(f\"\\nðŸ” Phase 4: Extracting Rules from Specifications...\")\n","            # preprocessor = EnhancedSpecPreprocessor()\n","            # all_rules = []\n","\n","            # for spec_key, spec_data in specifications.items():\n","            #     print(f\"   Processing {spec_key}...\")\n","            #     rules = preprocessor.extract_rules(spec_data)\n","            #     all_rules.extend(rules)\n","            #     print(f\"   Extracted {len(rules)} rules\")\n","\n","            # print(f\"âœ… Total rules extracted: {len(all_rules)}\")\n","\n","            # # Save raw rules\n","            file_path = f'{self.project_path}/data/extracted_rules.json'\n","            # JSONArrayManager.save_json_array(file_path, all_rules)\n","\n","            all_rules = JSONArrayManager.load_json_array(file_path)\n","\n","            # # Phase 5: Analyze rules with LLM\n","            # print(f\"\\nðŸ¤– Phase 5: Analyzing Rules with LLM...\")\n","            # analyzer = EnhancedLLMRuleAnalyzer(llm_client)\n","            # processed_rules = []\n","\n","            # # Limit rule processing in test mode\n","            # rules_to_process = all_rules[:20] if self.mode == 'test' else all_rules\n","            # print(f\"Processing {len(rules_to_process)} rules...\")\n","\n","            # for i, rule in enumerate(rules_to_process):\n","            #     try:\n","            #         print(f\"   Analyzing rule {i+1}/{len(rules_to_process)}...\")\n","\n","            #         # Categorize rule\n","            #         section_info = f\"{rule['section_title']} ({rule['section_number']})\"\n","            #         category = analyzer.categorize_rule(\n","            #             rule['text'],\n","            #             rule['java_version'],\n","            #             section_info\n","            #         )\n","\n","            #         # Extract components\n","            #         components = analyzer.extract_rule_components(\n","            #             rule['text'],\n","            #             rule['java_version']\n","            #         )\n","\n","            #         # Convert to formal logic\n","            #         formal_logic = analyzer.convert_to_formal_logic(\n","            #             rule['text'],\n","            #             components,\n","            #             rule['java_version']\n","            #         )\n","\n","            #         # Create processed rule\n","            #         processed_rule = {\n","            #             **rule,\n","            #             'category': category,\n","            #             'components': components,\n","            #             'formal_logic': formal_logic,\n","            #             'processed_at': datetime.now().isoformat()\n","            #         }\n","\n","            #         processed_rules.append(processed_rule)\n","\n","            #         # Delay between API calls\n","            #         time.sleep(1 if self.mode == 'test' else 0.5)\n","\n","            #     except Exception as e:\n","            #         print(f\"   âš ï¸ Error processing rule {i+1}: {e}\")\n","            #         continue\n","\n","            # print(f\"âœ… Processed {len(processed_rules)} rules\")\n","\n","            # Save processed rules\n","            file_path = f'{self.project_path}/data/processed_rules.json'\n","            # JSONArrayManager.save_json_array(file_path, processed_rules)\n","\n","            processed_rules = JSONArrayManager.load_json_array(file_path)\n","\n","            # Validate processed_rules structure\n","            if not isinstance(processed_rules, list):\n","                raise ValueError(\"processed_rules must be a list\")\n","\n","            # Phase 6: Detect conflicts\n","            print(f\"\\nðŸ” Phase 6: Detecting Conflicts...\")\n","            conflict_detector = EnhancedConflictDetector(llm_client, self.mode, self.project_path, self.max_llm_calls)\n","\n","            # Add all processed rules to conflict detector\n","            for i, rule in enumerate(processed_rules):\n","                try:\n","                    # Ensure rule is properly formatted and hashable\n","                    if not isinstance(rule, dict):\n","                        print(f\"âš ï¸ Warning: Rule {i} is not a dictionary, skipping...\")\n","                        continue\n","\n","                    # Convert any list values to tuples for hashability if needed\n","                    cleaned_rule = self._clean_rule_for_hashing(rule)\n","                    conflict_detector.add_rule(cleaned_rule)\n","\n","                except Exception as rule_error:\n","                    print(f\"âš ï¸ Warning: Failed to add rule {i}: {rule_error}\")\n","                    continue\n","\n","            # Find conflicts\n","            conflicts = conflict_detector.find_potential_conflicts()\n","\n","            # Generate conflict report\n","            conflict_report = conflict_detector.generate_conflict_report()\n","\n","            # Save conflict report\n","            file_path = f'{self.project_path}/results/conflict_report.json'\n","            JSONArrayManager.save_single_object(file_path, conflict_report)\n","\n","            # Phase 7: Generate final summary\n","            print(f\"\\nðŸ“Š Phase 7: Generating Summary Report...\")\n","            summary = self._generate_final_summary(\n","                specifications, all_rules, processed_rules, conflict_report, start_time\n","            )\n","\n","            # Save summary\n","            file_path = f'{self.project_path}/results/final_summary.json'\n","            JSONArrayManager.save_single_object(file_path, summary)\n","\n","            # Display results\n","            self._display_results(summary, conflict_report)\n","\n","            return summary\n","\n","        except Exception as e:\n","            print(f\"âŒ Analysis failed: {e}\")\n","            import traceback\n","            traceback.print_exc()\n","            return None\n","\n","    def _clean_rule_for_hashing(self, rule):\n","        \"\"\"Clean rule data to make it hashable by converting lists to tuples\"\"\"\n","        if isinstance(rule, dict):\n","            cleaned = {}\n","            for key, value in rule.items():\n","                if isinstance(value, list):\n","                    # Convert list to tuple for hashability\n","                    cleaned[key] = tuple(value) if value else ()\n","                elif isinstance(value, dict):\n","                    cleaned[key] = self._clean_rule_for_hashing(value)\n","                else:\n","                    cleaned[key] = value\n","            return cleaned\n","        elif isinstance(rule, list):\n","            return tuple(rule)\n","        else:\n","            return rule\n","\n","    def _generate_final_summary(self, specifications, raw_rules, processed_rules, conflict_report, start_time):\n","        \"\"\"Generate comprehensive final summary\"\"\"\n","        end_time = datetime.now()\n","        duration = end_time - start_time\n","\n","        # Analyze rule distribution by category\n","        category_distribution = {}\n","        for rule in processed_rules:\n","            category = rule.get('category', 'UNKNOWN')\n","            category_distribution[category] = category_distribution.get(category, 0) + 1\n","\n","        # Analyze rules by Java version\n","        version_distribution = {}\n","        for rule in processed_rules:\n","            version = rule.get('java_version', 'unknown')\n","            version_distribution[version] = version_distribution.get(version, 0) + 1\n","\n","        summary = {\n","            'analysis_info': {\n","                'mode': self.mode,\n","                'start_time': start_time.isoformat(),\n","                'end_time': end_time.isoformat(),\n","                'duration_seconds': duration.total_seconds(),\n","                'duration_formatted': str(duration)\n","            },\n","            'data_statistics': {\n","                'specifications_downloaded': len(specifications),\n","                'raw_rules_extracted': len(raw_rules),\n","                'rules_processed': len(processed_rules),\n","                'processing_success_rate': f\"{len(processed_rules)/max(len(raw_rules), 1)*100:.1f}%\"\n","            },\n","            'rule_analysis': {\n","                'category_distribution': category_distribution,\n","                'version_distribution': version_distribution,\n","                'top_categories': sorted(category_distribution.items(), key=lambda x: x[1], reverse=True)[:5]\n","            },\n","            'conflict_analysis': {\n","                'total_conflicts': conflict_report.get('total_conflicts', 0),\n","                'by_severity': conflict_report.get('by_severity', {}),\n","                'by_type': conflict_report.get('by_type', {}),\n","                'critical_conflicts': [c for c in conflict_report.get('detailed_conflicts', [])\n","                                     if c.get('severity') == 'HIGH']\n","            },\n","            'recommendations': conflict_report.get('recommendations', []),\n","            'files_generated': [\n","                f'{self.project_path}/data/all_specifications.json',\n","                f'{self.project_path}/data/extracted_rules.json',\n","                f'{self.project_path}/data/processed_rules.json',\n","                f'{self.project_path}/results/conflict_report.json',\n","                f'{self.project_path}/results/final_summary.json'\n","            ]\n","        }\n","\n","        return summary\n","\n","    def _display_results(self, summary, conflict_report):\n","        \"\"\"Display final results in a formatted way\"\"\"\n","        print(f\"\\n{'='*80}\")\n","        print(f\"ðŸŽ¯ SPECSENTINEL ANALYSIS COMPLETE ({self.mode.upper()} MODE)\")\n","        print(f\"{'='*80}\")\n","\n","        # Analysis Statistics\n","        print(f\"\\nðŸ“Š Analysis Statistics:\")\n","        print(f\"   Duration: {summary['analysis_info']['duration_formatted']}\")\n","        print(f\"   Specifications: {summary['data_statistics']['specifications_downloaded']}\")\n","        print(f\"   Rules Extracted: {summary['data_statistics']['raw_rules_extracted']}\")\n","        print(f\"   Rules Processed: {summary['data_statistics']['rules_processed']}\")\n","        print(f\"   Success Rate: {summary['data_statistics']['processing_success_rate']}\")\n","\n","        # Rule Categories\n","        print(f\"\\nðŸ“š Top Rule Categories:\")\n","        for category, count in summary['rule_analysis']['top_categories']:\n","            print(f\"   {category}: {count} rules\")\n","\n","        # Conflict Summary\n","        print(f\"\\nðŸ” Conflict Analysis:\")\n","        print(f\"   Total Conflicts: {conflict_report['total_conflicts']}\")\n","\n","        if conflict_report['by_severity']:\n","            print(f\"   By Severity:\")\n","            for severity, count in conflict_report['by_severity'].items():\n","                print(f\"      {severity}: {count}\")\n","\n","        if conflict_report['by_type']:\n","            print(f\"   By Type:\")\n","            for conf_type, count in conflict_report['by_type'].items():\n","                print(f\"      {conf_type}: {count}\")\n","\n","        # Recommendations\n","        if conflict_report['recommendations']:\n","            print(f\"\\nðŸ’¡ Key Recommendations:\")\n","            for i, rec in enumerate(conflict_report['recommendations'][:5], 1):\n","                print(f\"   {i}. {rec}\")\n","\n","        # Files Generated\n","        print(f\"\\nðŸ“ Generated Files:\")\n","        for file_path in summary['files_generated']:\n","            if os.path.exists(file_path):\n","                size = os.path.getsize(file_path)\n","                print(f\"   âœ… {os.path.basename(file_path)} ({size:,} bytes)\")\n","            else:\n","                print(f\"   âŒ {os.path.basename(file_path)} (missing)\")\n","\n","        print(f\"\\nðŸŽ‰ Analysis complete! Results saved to: {self.project_path}\")\n","\n","    def export_for_agent(self):\n","        '''Export data in format suitable for the agent'''\n","        agent_data = {\n","            'specifications': self.specifications,\n","            'processed_rules': self.processed_rules,\n","            'conflicts': self.conflict_report,\n","            'summary': self.summary\n","        }\n","\n","        export_path = f'{self.project_path}/agent_knowledge_base.json'\n","        with open(export_path, 'w', encoding='utf-8') as f:\n","            json.dump(agent_data, f, indent=2, ensure_ascii=False)\n","\n","        print(f\"âœ… Agent knowledge base exported to: {export_path}\")\n","\n","# =============================================================================\n","# MAIN EXECUTION FUNCTION\n","# =============================================================================\n","\n","def main():\n","    \"\"\"Main execution function with mode selection\"\"\"\n","    # Phase 0: Mode selection\n","    mode = select_run_mode()\n","    MAX_LLM_CALLS = 15000\n","\n","    if mode is None:\n","        print(\"Analysis cancelled.\")\n","        return\n","\n","    shall_get_custom_section_choice = False\n","\n","    if mode == 1:\n","      shall_get_custom_section_choice = get_custom_section_choice()\n","\n","    if shall_get_custom_section_choice is None:\n","        print(\"Analysis cancelled.\")\n","        return\n","\n","    custom_section_details = None\n","\n","    if shall_get_custom_section_choice:\n","        custom_section_details = get_custom_section_details()\n","\n","    # Initialize and run orchestrator\n","    orchestrator = SpecSentinelOrchestrator(mode, custom_section_details, MAX_LLM_CALLS)\n","    # Store specifications, processed_rules, conflicts, and summary as attributes\n","    # so they can be accessed by export_for_agent\n","    orchestrator.run_complete_analysis()\n","\n","    # Access the results directly from the orchestrator instance\n","    analysis_results = orchestrator.results\n","\n","    if analysis_results:\n","        print(f\"\\nâœ… SpecSentinel analysis completed successfully in {mode} mode!\")\n","        # Export for agent\n","        orchestrator.export_for_agent()\n","\n","        # Decide which interface to run based on user input or config\n","        # If you want to launch Streamlit automatically after analysis:\n","        print(\"\\nðŸš€ Launching Streamlit Interface...\")\n","        # Assuming streamlit_app.py is saved in the project_path\n","        streamlit_script_path = f'{orchestrator.project_path}/streamlit_app.py'\n","        # Use nohup and & to run in the background\n","        # Use npx localtunnel to expose the port\n","        !nohup streamlit run {streamlit_script_path} & npx localtunnel --port 8501 &\n","\n","        # If you wanted the Colab text interface instead, you would call:\n","        # run_colab_interface()\n","\n","    else:\n","        print(f\"\\nâŒ SpecSentinel analysis failed.\")\n","\n","# Ensure the main function is called\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","source":["# Cell to get the public IP address of the Colab instance\n","!curl ifconfig.me"],"metadata":{"id":"hK2cTHuYVZVf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Install required packages\n","print(\"ðŸ“¦ Installing dependencies...\")\n","!pip install -q transformers torch\n","!pip install -q sentence-transformers\n","!pip install -q spacy nltk beautifulsoup4 lxml\n","!pip install -q z3-solver sympy\n","!pip install -q openai httpx requests\n","!pip install -q PyPDF2 pdfplumber\n","!pip install -q python-dotenv\n","!pip install streamlit\n","\n","# Download spaCy model\n","!python -m spacy download en_core_web_sm\n","\n","from google.colab import drive\n","\n","# Define project path\n","project_path = '/content/drive/My Drive/SpecSentinel'\n","\n","def setup_project_path(mode='full'):\n","    \"\"\"Setup environment based on mode\"\"\"\n","    print(f\"ðŸš€ SpecSentinel MVP - Setting up project structure ({mode} mode)...\")\n","\n","    # Check if Drive is already mounted\n","    if not os.path.exists('/content/drive'):\n","        print(\"ðŸ“‚ Mounting Google Drive...\")\n","        drive.mount('/content/drive')\n","    else:\n","        print(\"ðŸ“‚ Google Drive already mounted\")\n","\n","    # Check if SpecSentinel folder exists and return early if it does\n","    if os.path.exists(project_path):\n","        print(f\"ðŸ“ Project directory already exists at: {project_path}\")\n","        return project_path\n","\n","    # Create project directory structure\n","    print(\"ðŸ—ï¸ Creating project directory structure...\")\n","    os.makedirs(f'{project_path}/data', exist_ok=True)\n","    os.makedirs(f'{project_path}/models', exist_ok=True)\n","    os.makedirs(f'{project_path}/results', exist_ok=True)\n","    os.makedirs(f'{project_path}/logs', exist_ok=True)\n","\n","    print(f\"âœ… Project directory created at: {project_path}\")\n","\n","    return project_path\n","\n","# New Cell in your Colab Notebook\n","\n","# Install localtunnel if not already installed in dependencies cell\n","!npm install localtunnel\n","\n","# Assuming streamlit_app.py is in your project_path\n","import os\n","project_path = setup_project_path()\n","streamlit_script_path = os.path.join(project_path, 'streamlit_app_with_graph.py')\n","\n","# Change directory to the project path so streamlit can find the script easily\n","%cd /content/drive/My Drive/SpecSentinel/\n","\n","# Run Streamlit in the background and expose it via localtunnel\n","!nohup env STREAMLIT_SERVER_FILE_WATCHER_TYPE=\"none\" streamlit run \"{streamlit_script_path}\" & npx localtunnel --port 8501 &\n","# You should see a public URL generated by localtunnel that you can open in your browser.\n","# To stop Streamlit, you might need to find its process ID and kill it.\n","# Alternatively, restart the Colab runtime."],"metadata":{"id":"ZU-TGGKnSSWp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell to view the full content of nohup.out\n","!cat '/content/drive/My Drive/SpecSentinel/nohup.out'"],"metadata":{"id":"BfHEdpwLaB-l"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNT7zsNbTw+rhit8ud2qxQE"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}