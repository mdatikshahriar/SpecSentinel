{"cells":[{"cell_type":"code","source":["import requests\n","import json\n","from google.colab import userdata\n","\n","key = userdata.get(\"OPENROUTER_API_KEY\")\n","response = requests.get(\n","  url=\"https://openrouter.ai/api/v1/auth/key\",\n","  headers={\n","    \"Authorization\": f\"Bearer {key}\"\n","  }\n",")\n","\n","print(\"key -> \" + json.dumps(response.json(), indent=2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3qpBiObXY07f","executionInfo":{"status":"ok","timestamp":1748346891857,"user_tz":-360,"elapsed":1741,"user":{"displayName":"Md. Atik Shahriar","userId":"05541666036680278842"}},"outputId":"39b0e6fa-4241-46fe-ed9a-4f2b6a7aaab4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["key -> {\n","  \"data\": {\n","    \"label\": \"sk-or-v1-d35...bb7\",\n","    \"limit\": 10,\n","    \"usage\": 0.45811,\n","    \"is_provisioning_key\": false,\n","    \"limit_remaining\": 9.54189,\n","    \"is_free_tier\": false,\n","    \"rate_limit\": {\n","      \"requests\": 100,\n","      \"interval\": \"10s\"\n","    }\n","  }\n","}\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cDtaQY7pQgLR","executionInfo":{"status":"ok","timestamp":1748339029145,"user_tz":-360,"elapsed":357483,"user":{"displayName":"Md. Atik Shahriar","userId":"05541666036680278842"}},"outputId":"40dca6a5-12b8-47c5-8002-66e003d6213e"},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ“¦ Installing dependencies...\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting en-core-web-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","âœ… Dependencies installed successfully!\n","ðŸš€ SpecSentinel MVP\n","==================================================\n","How do you want to run SpecSentinel?\n","1. Test Mode - Limited specifications, free models only\n","2. Full Mode - Complete analysis with all specifications\n","==================================================\n","Enter your choice (1 or 2): 1\n","\n","ðŸ” Test Mode Section Selection\n","========================================\n","Do you want to analyze a specific Java specification section?\n","1. Yes - I'll specify a custom section\n","2. No - Use default test sections\n","========================================\n","Enter your choice (1 or 2): 2\n","\n","============================================================\n","ðŸš€ STARTING SPECSENTINEL ANALYSIS (TEST MODE)\n","============================================================\n","ðŸš€ SpecSentinel MVP - Setting up project structure (test mode)...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Existing SpecSentinel folder deleted from /content/drive/My Drive/SpecSentinel\n","SpecSentinel project structure created at: /content/drive/My Drive/SpecSentinel\n","âœ… Project directory created at: /content/drive/My Drive/SpecSentinel\n","\n","ðŸ“¡ Phase 2: Initializing LLM Client...\n","ðŸ§ª Test mode: Using free models only\n","\n","ðŸ“š Phase 3: Downloading Java Specifications...\n","ðŸ§ª Test mode: Using default test section\n","ðŸ“Š Processing 1 section(s) across 2 Java versions\n","ðŸ“š Downloading 1 section(s) across 2 Java versions...\n","\n","ðŸ“– Processing: The switch Statement\n","   ðŸ“¥ Downloading Java 8 - switch_statement from https://docs.oracle.com/javase/specs/jls/se8/html/jls-14.html#jls-14.11\n","   âœ… Downloaded 2349 characters\n","   ðŸ“¥ Downloading Java 24 - switch_statement from https://docs.oracle.com/javase/specs/jls/se24/html/jls-14.html#jls-14.11\n","   âœ… Downloaded 2416 characters\n","\n","âœ… Downloaded 2 specification sections total\n","\n","ðŸ” Phase 4: Extracting Rules from Specifications...\n","   Processing switch_statement_java8...\n","   Extracted 10 rules\n","   Processing switch_statement_java24...\n","   Extracted 10 rules\n","âœ… Total rules extracted: 20\n","\n","ðŸ¤– Phase 5: Analyzing Rules with LLM...\n","Processing 20 rules...\n","   Analyzing rule 1/20...\n","   Analyzing rule 2/20...\n","   Analyzing rule 3/20...\n","   Analyzing rule 4/20...\n","   Analyzing rule 5/20...\n","   Analyzing rule 6/20...\n","   Analyzing rule 7/20...\n","   Analyzing rule 8/20...\n","   Analyzing rule 9/20...\n","   Analyzing rule 10/20...\n","   Analyzing rule 11/20...\n","   Analyzing rule 12/20...\n","   Analyzing rule 13/20...\n","   Analyzing rule 14/20...\n","   Analyzing rule 15/20...\n","   Analyzing rule 16/20...\n","   Analyzing rule 17/20...\n","   Analyzing rule 18/20...\n","   Analyzing rule 19/20...\n","   Analyzing rule 20/20...\n","âœ… Processed 20 rules\n","\n","ðŸ” Phase 6: Detecting Conflicts...\n","ðŸ§ª Test mode: Limiting to 50 rule comparisons\n","   ðŸ” Potential conflict found: VERSION_CHANGE between Java 8 and 24\n","   ðŸ” Potential conflict found: VERSION_CHANGE between Java 8 and 24\n","âœ… Found 2 potential conflicts\n","\n","ðŸ“Š Phase 7: Generating Summary Report...\n","\n","================================================================================\n","ðŸŽ¯ SPECSENTINEL ANALYSIS COMPLETE (TEST MODE)\n","================================================================================\n","\n","ðŸ“Š Analysis Statistics:\n","   Duration: 0:04:48.889142\n","   Specifications: 2\n","   Rules Extracted: 20\n","   Rules Processed: 20\n","   Success Rate: 100.0%\n","\n","ðŸ“š Top Rule Categories:\n","   EXCEPTION_HANDLING: 16 rules\n","   COMPILATION_RULES: 4 rules\n","\n","ðŸ” Conflict Analysis:\n","   Total Conflicts: 2\n","   By Severity:\n","      MEDIUM: 1\n","      LOW: 1\n","   By Type:\n","      VERSION_CHANGE: 2\n","\n","ðŸ’¡ Key Recommendations:\n","   1. ðŸ”„ 2 version changes require migration guidance\n","\n","ðŸ“ Generated Files:\n","   âœ… all_specifications.json (5,594 bytes)\n","   âœ… extracted_rules.json (110,175 bytes)\n","   âœ… processed_rules.json (126,684 bytes)\n","   âœ… conflict_report.json (3,384 bytes)\n","   âœ… final_summary.json (1,383 bytes)\n","\n","ðŸŽ‰ Analysis complete! Results saved to: /content/drive/My Drive/SpecSentinel\n","\n","âœ… SpecSentinel analysis completed successfully in test mode!\n"]}],"source":["# Version 2 - SpecSentinel MVP with Test Mode\n","\n","import os\n","import json\n","import re\n","import requests\n","import itertools\n","from datetime import datetime, timedelta\n","import pandas as pd\n","import numpy as np\n","from typing import List, Dict, Any, Optional\n","import time\n","import random\n","\n","# =============================================================================\n","# PHASE 0: MODE SELECTION WITH CUSTOM SECTION OPTION\n","# =============================================================================\n","\n","def select_run_mode():\n","    \"\"\"Allow user to select between test mode and full mode\"\"\"\n","    print(\"ðŸš€ SpecSentinel MVP\")\n","    print(\"=\"*50)\n","    print(\"How do you want to run SpecSentinel?\")\n","    print(\"1. Test Mode - Limited specifications, free models only\")\n","    print(\"2. Full Mode - Complete analysis with all specifications\")\n","    print(\"=\"*50)\n","\n","    while True:\n","        try:\n","            choice = input(\"Enter your choice (1 or 2): \").strip()\n","            if choice == '1':\n","                return 'test'\n","            elif choice == '2':\n","                return 'full'\n","            else:\n","                print(\"Please enter either 1 or 2\")\n","        except KeyboardInterrupt:\n","            print(\"\\nExiting...\")\n","            return None\n","\n","def get_custom_section_choice():\n","    \"\"\"Ask user if they want to analyze a custom section in test mode\"\"\"\n","    print(\"\\nðŸ” Test Mode Section Selection\")\n","    print(\"=\"*40)\n","    print(\"Do you want to analyze a specific Java specification section?\")\n","    print(\"1. Yes - I'll specify a custom section\")\n","    print(\"2. No - Use default test sections\")\n","    print(\"=\"*40)\n","\n","    while True:\n","        try:\n","            choice = input(\"Enter your choice (1 or 2): \").strip()\n","            if choice == '1':\n","                return True\n","            elif choice == '2':\n","                return False\n","            else:\n","                print(\"Please enter either 1 or 2\")\n","        except KeyboardInterrupt:\n","            print(\"\\nExiting...\")\n","            return None\n","\n","def get_custom_section_details():\n","    \"\"\"Get custom section details from user\"\"\"\n","    print(\"\\nðŸ“ Enter Custom Section Details\")\n","    print(\"=\"*35)\n","    print(\"Please provide the following information:\")\n","\n","    # Get section title\n","    while True:\n","        title = input(\"Section Title (e.g., 'The switch Statement'): \").strip()\n","        if title:\n","            break\n","        print(\"Please enter a valid section title.\")\n","\n","    # Get section number\n","    while True:\n","        section_num = input(\"Section Number (e.g., '14.11' or '8.4.8'): \").strip()\n","        if section_num and re.match(r'^\\d+(\\.\\d+)*$', section_num):\n","            break\n","        print(\"Please enter a valid section number (e.g., '14.11' or '8.4.8').\")\n","\n","    # Create section key from title\n","    section_key = title.lower().replace(' ', '_').replace('(', '').replace(')', '').replace(',', '')\n","    section_key = re.sub(r'[^a-z0-9_]', '', section_key)\n","\n","    return {\n","        'key': section_key,\n","        'title': title,\n","        'section_number': section_num\n","    }\n","\n","# =============================================================================\n","# PHASE 1: SETUP AND DEPENDENCIES\n","# =============================================================================\n","\n","# Install required packages\n","print(\"ðŸ“¦ Installing dependencies...\")\n","!pip install -q transformers torch\n","!pip install -q sentence-transformers\n","!pip install -q spacy nltk beautifulsoup4 lxml\n","!pip install -q z3-solver sympy\n","!pip install -q openai httpx requests\n","!pip install -q PyPDF2 pdfplumber\n","!pip install -q python-dotenv\n","!pip install streamlit\n","\n","# Download spaCy model\n","!python -m spacy download en_core_web_sm\n","\n","# Import all necessary libraries\n","import spacy\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","from bs4 import BeautifulSoup\n","from z3 import *\n","import openai\n","from google.colab import userdata\n","import pickle\n","from sentence_transformers import SentenceTransformer\n","import streamlit as st\n","\n","# Download NLTK data\n","nltk.download('punkt', quiet=True)\n","nltk.download('punkt_tab', quiet=True)\n","nltk.download('stopwords', quiet=True)\n","\n","print(\"âœ… Dependencies installed successfully!\")\n","\n","# Define project path\n","project_path = '/content/drive/My Drive/SpecSentinel'\n","\n","def setup_project_path(mode='full'):\n","    \"\"\"Setup environment based on mode\"\"\"\n","    print(f\"ðŸš€ SpecSentinel MVP - Setting up project structure ({mode} mode)...\")\n","\n","    # Mount Google Drive\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    # Check if SpecSentinel folder exists and delete it if it does\n","    if os.path.exists(project_path):\n","        return project_path\n","\n","    # Create project directory structure\n","    os.makedirs(f'{project_path}/data', exist_ok=True)\n","    os.makedirs(f'{project_path}/models', exist_ok=True)\n","    os.makedirs(f'{project_path}/results', exist_ok=True)\n","    os.makedirs(f'{project_path}/logs', exist_ok=True)\n","\n","    print(f\"SpecSentinel project structure created at: {project_path}\")\n","    print(f\"âœ… Project directory created at: {project_path}\")\n","\n","    return project_path\n","\n","# =============================================================================\n","# PHASE 2: OPENROUTER API SETUP WITH MODE-AWARE MODEL SELECTION\n","# =============================================================================\n","\n","class EnhancedOpenRouterClient:\n","    def __init__(self, mode='full'):\n","        self.mode = mode\n","\n","        # Get API keys from Colab secrets (supports multiple keys)\n","        self.api_keys = []\n","        try:\n","            # Try to get multiple API keys\n","            for i in range(1, 6):  # Support up to 5 keys\n","                key_name = f'OPENROUTER_API_KEY_{i}' if i > 1 else 'OPENROUTER_API_KEY'\n","                key = os.getenv(key_name)\n","                try:\n","                    key = userdata.get(key_name)\n","                    if key:\n","                        self.api_keys.append(key)\n","                except:\n","                    break\n","\n","            if not self.api_keys:\n","                raise ValueError(\"No API keys found\")\n","\n","        except Exception as e:\n","            # Fallback: Ask user to input API keys\n","            from getpass import getpass\n","            print(f\"No API keys found in secrets. Please enter your OpenRouter API keys:\")\n","            key = getpass(\"Enter your primary OpenRouter API key: \")\n","            self.api_keys.append(key)\n","\n","            # In test mode, we only need one API key for free models\n","            if mode == 'full':\n","                while True:\n","                    additional = input(\"Do you have additional API keys? (y/n): \").lower()\n","                    if additional == 'y':\n","                        key = getpass(\"Enter additional API key: \")\n","                        self.api_keys.append(key)\n","                    else:\n","                        break\n","\n","        self.current_key_idx = 0\n","\n","        # Model selection based on mode\n","        if mode == 'test':\n","            # Test mode: Only free models\n","            self.models = [\n","                \"anthropic/claude-3.5-sonnet-20241022\",\n","                \"openai/gpt-4o-2024-11-20\",\n","                \"deepseek/deepseek-chat-v3-0324:free\",\n","                \"meta-llama/llama-3.2-3b-instruct:free\",\n","                \"google/gemma-2-9b-it:free\",\n","                \"microsoft/phi-3-mini-128k-instruct:free\"\n","            ]\n","            print(\"ðŸ§ª Test mode: Using free models only\")\n","        else:\n","            # Full mode: model hierarchy (most powerful to least powerful)\n","            self.models = [\n","                # Most Powerful Models\n","                \"anthropic/claude-3.5-sonnet-20241022\",\n","                \"openai/gpt-4o-2024-11-20\",\n","                \"google/gemini-2.0-flash-thinking-exp\",\n","                \"anthropic/claude-3.5-haiku-20241022\",\n","                \"openai/gpt-4o-mini-2024-07-18\",\n","                \"google/gemini-2.0-flash-exp\",\n","                \"meta-llama/llama-3.3-70b-instruct\",\n","                # Free Models\n","                \"deepseek/deepseek-chat-v3-0324:free\",\n","                \"meta-llama/llama-3.2-3b-instruct:free\",\n","                \"google/gemma-2-9b-it:free\",\n","                \"microsoft/phi-3-mini-128k-instruct:free\"\n","            ]\n","            print(\"ðŸš€ Full mode: Using all available models\")\n","\n","        self.current_model_idx = 0\n","        self.model_cooldowns = {}  # Track cooldown periods for models\n","        self.model_error_counts = {}  # Track error counts per model\n","\n","    def get_current_api_key(self) -> str:\n","        \"\"\"Get current API key with rotation\"\"\"\n","        return self.api_keys[self.current_key_idx % len(self.api_keys)]\n","\n","    def rotate_api_key(self):\n","        \"\"\"Rotate to next API key\"\"\"\n","        self.current_key_idx = (self.current_key_idx + 1) % len(self.api_keys)\n","\n","    def is_model_in_cooldown(self, model: str) -> bool:\n","        \"\"\"Check if model is in cooldown period\"\"\"\n","        if model not in self.model_cooldowns:\n","            return False\n","\n","        cooldown_until = self.model_cooldowns[model]\n","        return datetime.now() < cooldown_until\n","\n","    def set_model_cooldown(self, model: str):\n","        \"\"\"Set cooldown period for model based on whether it's free or paid\"\"\"\n","        is_free_model = ':free' in model\n","        # Longer cooldowns in test mode to be more conservative with free models\n","        if self.mode == 'test':\n","            cooldown_seconds = 15 if is_free_model else 90\n","        else:\n","            cooldown_seconds = 10 if is_free_model else 60\n","\n","        self.model_cooldowns[model] = datetime.now() + timedelta(seconds=cooldown_seconds)\n","        print(f\"â³ Model {model} in cooldown for {cooldown_seconds} seconds\")\n","\n","    def call_llm(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.3) -> str:\n","        \"\"\"LLM calling with improved error handling and API key rotation\"\"\"\n","        available_models = [m for m in self.models if not self.is_model_in_cooldown(m)]\n","\n","        if not available_models:\n","            print(\"âš ï¸ All models in cooldown, waiting...\")\n","            wait_time = 15 if self.mode == 'test' else 5\n","            time.sleep(wait_time)\n","            available_models = self.models\n","\n","        for model in available_models:\n","            for attempt in range(len(self.api_keys)):\n","                try:\n","                    current_key = self.get_current_api_key()\n","\n","                    response = requests.post(\n","                        \"https://openrouter.ai/api/v1/chat/completions\",\n","                        headers={\n","                            \"Authorization\": f\"Bearer {current_key}\",\n","                            \"Content-Type\": \"application/json\",\n","                            \"HTTP-Referer\": \"https://github.com/your-repo\",\n","                            \"X-Title\": \"SpecSentinel\"\n","                        },\n","                        json={\n","                            \"model\": model,\n","                            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n","                            \"max_tokens\": max_tokens,\n","                            \"temperature\": temperature\n","                        },\n","                        timeout=30\n","                    )\n","\n","                    if response.status_code == 200:\n","                        result = response.json()\n","                        return result['choices'][0]['message']['content']\n","                    elif response.status_code == 429:  # Rate limit\n","                        print(f\"ðŸ”„ Rate limit for {model} with key {attempt+1}, rotating...\")\n","                        self.rotate_api_key()\n","                        wait_time = 5 if self.mode == 'test' else 2\n","                        time.sleep(wait_time)\n","                        continue\n","                    elif response.status_code == 401:  # Auth error\n","                        print(f\"ðŸ”‘ Auth error with key {attempt+1}, rotating...\")\n","                        self.rotate_api_key()\n","                        continue\n","                    else:\n","                        print(f\"âŒ Model {model} failed with status {response.status_code}\")\n","                        break\n","\n","                except requests.exceptions.Timeout:\n","                    print(f\"â° Timeout for {model}, trying next...\")\n","                    break\n","                except Exception as e:\n","                    print(f\"âŒ Model {model} failed: {str(e)[:100]}\")\n","                    break\n","\n","            # Set cooldown for failed model\n","            self.set_model_cooldown(model)\n","\n","        raise Exception(\"All models and API keys failed\")\n","\n","# =============================================================================\n","# PHASE 3: ENHANCED SPECIFICATION DATA ACQUISITION WITH CUSTOM SECTION SUPPORT\n","# =============================================================================\n","\n","class MultiVersionSpecificationDownloader:\n","    def __init__(self, mode='full', custom_section=None):\n","        self.mode = mode\n","        self.custom_section = custom_section\n","\n","        # Full specification sections\n","        full_spec_sections = {\n","            \"method_resolution\": {\n","                \"description\": \"Method Resolution and Invocation\",\n","                \"section\": \"15.12\"\n","            },\n","            \"switch_statement\": {\n","                \"description\": \"The switch Statement\",\n","                \"section\": \"14.11\"\n","            },\n","            \"inheritance\": {\n","                \"description\": \"Inheritance and Method Overriding\",\n","                \"section\": \"8.4.8\"\n","            },\n","            \"overloading\": {\n","                \"description\": \"Method Overloading Resolution\",\n","                \"section\": \"15.12.2\"\n","            },\n","            \"generics\": {\n","                \"description\": \"Generic Types and Type Parameters\",\n","                \"section\": \"4.5\"\n","            },\n","            \"exceptions\": {\n","                \"description\": \"Exception Handling\",\n","                \"section\": \"11\"\n","            },\n","            \"interfaces\": {\n","                \"description\": \"Interface Declarations\",\n","                \"section\": \"9\"\n","            },\n","            \"abstract_methods\": {\n","                \"description\": \"Abstract Method Declarations\",\n","                \"section\": \"8.4.3\"\n","            },\n","            \"constructors\": {\n","                \"description\": \"Constructor Declarations\",\n","                \"section\": \"8.8\"\n","            }\n","        }\n","\n","        # Test mode: Default sections or custom section\n","        if mode == 'test':\n","            if custom_section:\n","                # Use custom section provided by user\n","                self.spec_sections = {\n","                    custom_section['key']: {\n","                        \"description\": custom_section['title'],\n","                        \"section\": custom_section['section_number']\n","                    }\n","                }\n","                print(f\"ðŸŽ¯ Test mode: Analyzing custom section '{custom_section['title']}' (Section {custom_section['section_number']})\")\n","            else:\n","                # Use default test section\n","                test_spec_sections = {\n","                    \"switch_statement\": {\n","                        \"description\": \"The switch Statement\",\n","                        \"section\": \"14.11\"\n","                    }\n","                }\n","                self.spec_sections = test_spec_sections\n","                print(f\"ðŸ§ª Test mode: Using default test section\")\n","        else:\n","            # Full mode: All sections\n","            self.spec_sections = full_spec_sections\n","\n","        # Java versions based on mode\n","        if mode == 'test':\n","            # Test mode: Only 2 versions\n","            self.java_versions = {\n","                \"8\": \"se8\",\n","                \"24\": \"se24\"\n","            }\n","            print(f\"ðŸ“Š Processing {len(self.spec_sections)} section(s) across 2 Java versions\")\n","        else:\n","            # Full mode: All versions\n","            self.java_versions = {\n","                \"8\": \"se8\",\n","                \"11\": \"se11\",\n","                \"17\": \"se17\",\n","                \"21\": \"se21\",\n","                \"24\": \"se24\"\n","            }\n","            print(f\"ðŸš€ Full mode: Processing {len(self.spec_sections)} sections across {len(self.java_versions)} Java versions\")\n","\n","        self.session = requests.Session()\n","        self.session.headers.update({\n","            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n","        })\n","\n","    def build_url(self, java_version: str, section: str) -> str:\n","        \"\"\"Build JLS URL for specific version and section\"\"\"\n","        version_code = self.java_versions.get(java_version, \"se17\")\n","        base_url = f\"https://docs.oracle.com/javase/specs/jls/{version_code}/html\"\n","\n","        # Convert section number to JLS format\n","        if '.' in section:\n","            major, minor = section.split('.', 1)\n","            return f\"{base_url}/jls-{major}.html#jls-{section}\"\n","        else:\n","            return f\"{base_url}/jls-{section}.html\"\n","\n","    def escape_css_selector(self, selector: str) -> str:\n","        \"\"\"Escape dots in CSS selectors for BeautifulSoup\"\"\"\n","        # Replace dots with escaped dots for CSS selectors\n","        return selector.replace('.', '\\\\.')\n","\n","    def extract_section_info(self, soup: BeautifulSoup, java_version: str, section: str) -> Dict[str, str]:\n","        \"\"\"Extract section title and number from HTML\"\"\"\n","        section_info = {\n","            'java_version': java_version,\n","            'section_number': section,\n","            'section_title': 'Unknown',\n","            'chapter_title': 'Unknown'\n","        }\n","\n","        try:\n","            # Escape the section number for CSS selectors\n","            escaped_section = self.escape_css_selector(section)\n","\n","            # Try to find section title with properly escaped selectors\n","            title_selectors = [\n","                f'h2[id=\"jls-{section}\"]',  # Use attribute selector instead\n","                f'h3[id=\"jls-{section}\"]',\n","                f'h1[id=\"jls-{section}\"]',\n","                f'*[id=\"jls-{section}\"]',   # Any element with the ID\n","                '.section-title',\n","                '.chapter-title h1',\n","                'h1', 'h2'\n","            ]\n","\n","            for selector in title_selectors:\n","                try:\n","                    title_elem = soup.select_one(selector)\n","                    if title_elem:\n","                        title_text = title_elem.get_text(strip=True)\n","                        if section in title_text or any(word in title_text.lower() for word in ['method', 'class', 'interface', 'type']):\n","                            section_info['section_title'] = title_text\n","                            break\n","                except Exception as selector_error:\n","                    # Skip invalid selectors and continue\n","                    continue\n","\n","            # Try to find chapter title\n","            try:\n","                chapter_elem = soup.select_one('h1')\n","                if chapter_elem:\n","                    section_info['chapter_title'] = chapter_elem.get_text(strip=True)\n","            except Exception:\n","                pass\n","\n","        except Exception as e:\n","            print(f\"Warning: Could not extract section info: {e}\")\n","\n","        return section_info\n","\n","    def download_section(self, java_version: str, section_name: str, section_number: str) -> Optional[Dict[str, Any]]:\n","        \"\"\"Download and extract text from JLS section for specific Java version\"\"\"\n","        url = self.build_url(java_version, section_number)\n","\n","        try:\n","            print(f\"   ðŸ“¥ Downloading Java {java_version} - {section_name} from {url}\")\n","\n","            response = self.session.get(url, timeout=30)\n","            response.raise_for_status()\n","\n","            soup = BeautifulSoup(response.content, 'html.parser')\n","\n","            # Remove script and style elements\n","            for element in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\"]):\n","                element.decompose()\n","\n","            # Extract section metadata\n","            section_info = self.extract_section_info(soup, java_version, section_number)\n","\n","            # Extract main content using attribute selectors instead of ID selectors with dots\n","            content_selectors = [\n","                f'div[id=\"jls-{section_number}\"]',  # Use attribute selector\n","                f'*[id=\"jls-{section_number}\"]',    # Any element with the ID\n","                'div.section',\n","                'div.chapter',\n","                'main',\n","                'div.content',\n","                'article',\n","                'div[role=\"main\"]',\n","                'body'\n","            ]\n","\n","            content = None\n","            for selector in content_selectors:\n","                try:\n","                    content = soup.select_one(selector)\n","                    if content and content.get_text(strip=True):\n","                        break\n","                except Exception:\n","                    # Skip invalid selectors\n","                    continue\n","\n","            if not content:\n","                # Fallback to the entire soup\n","                content = soup\n","\n","            text = content.get_text(separator=' ', strip=True)\n","\n","            # Clean up the text\n","            text = re.sub(r'\\s+', ' ', text)\n","            text = re.sub(r'\\n+', '\\n', text)\n","\n","            # Remove common navigation text\n","            text = re.sub(r'(Contents|Previous|Next|Index)(\\s+\\||\\s+)?', '', text)\n","            text = re.sub(r'Oracle and/or its affiliates.*?All rights reserved\\.', '', text)\n","\n","            if len(text) < 100:\n","                print(f\"   âš ï¸ Very short content ({len(text)} chars), might be an error\")\n","                return None\n","\n","            result = {\n","                'text': text,\n","                'metadata': section_info,\n","                'url': url,\n","                'section_name': section_name,\n","                'downloaded_at': datetime.now().isoformat()\n","            }\n","\n","            print(f\"   âœ… Downloaded {len(text)} characters\")\n","            return result\n","\n","        except requests.exceptions.RequestException as e:\n","            print(f\"   âŒ Network error downloading {url}: {e}\")\n","            return None\n","        except Exception as e:\n","            print(f\"   âŒ Error processing {url}: {e}\")\n","            return None\n","\n","    def download_all_sections(self) -> Dict[str, Dict[str, Any]]:\n","        \"\"\"Download all JLS sections for selected Java versions\"\"\"\n","        all_sections = {}\n","\n","        print(f\"ðŸ“š Downloading {len(self.spec_sections)} section(s) across {len(self.java_versions)} Java versions...\")\n","\n","        for section_name, section_config in self.spec_sections.items():\n","            section_number = section_config['section']\n","            description = section_config['description']\n","\n","            print(f\"\\nðŸ“– Processing: {description}\")\n","\n","            for java_version in self.java_versions.keys():\n","                key = f\"{section_name}_java{java_version}\"\n","\n","                section_data = self.download_section(java_version, section_name, section_number)\n","\n","                if section_data:\n","                    all_sections[key] = section_data\n","\n","                    # Save individual section to file\n","                    filename = f'{project_path}/data/{key}.json'\n","                    with open(filename, 'w', encoding='utf-8') as f:\n","                        json.dump(section_data, f, indent=2, ensure_ascii=False)\n","                else:\n","                    print(f\"   âš ï¸ Failed to download Java {java_version} {section_name}\")\n","\n","                # Longer delay in test mode to be respectful with free models\n","                delay = 2 if self.mode == 'test' else 1\n","                time.sleep(delay)\n","\n","        print(f\"\\nâœ… Downloaded {len(all_sections)} specification sections total\")\n","        return all_sections\n","\n","# =============================================================================\n","# PHASE 4: RULE EXTRACTION WITH VERSION TRACKING\n","# =============================================================================\n","\n","class EnhancedSpecPreprocessor:\n","    def __init__(self):\n","        self.nlp = spacy.load('en_core_web_sm')\n","\n","        # rule patterns with more comprehensive detection\n","        self.rule_patterns = [\n","            r'\\b(must|shall|should|will|may|cannot|must not|may not|required to|prohibited from)\\b',\n","            r'\\b(if.*then|when.*then|unless|provided that|except when)\\b',\n","            r'\\b(required|mandatory|optional|forbidden|prohibited|allowed|permitted)\\b',\n","            r'\\b(always|never|only when|only if|if and only if|whenever)\\b',\n","            r'\\b(compile.time error|runtime error|exception|compilation error)\\b',\n","            r'\\b(every|all|any|some|no|none)\\b.*\\b(method|class|interface|type|variable)\\b',\n","            r'\\b(override|overload|implement|extend|inherit)\\b',\n","            r'\\b(accessible|visible|private|protected|public|package.private)\\b'\n","        ]\n","\n","    def extract_sentences(self, text: str) -> List[str]:\n","        \"\"\"Extract sentences from specification text with better filtering\"\"\"\n","        # Split into sentences\n","        sentences = sent_tokenize(text)\n","\n","        # Filter and clean sentences\n","        cleaned_sentences = []\n","        for sent in sentences:\n","            # Clean whitespace\n","            sent = re.sub(r'\\s+', ' ', sent).strip()\n","\n","            # Skip very short sentences, page numbers, navigation text\n","            if len(sent) < 15:\n","                continue\n","\n","            # Skip sentences that are likely navigation or formatting\n","            skip_patterns = [\n","                r'^\\d+(\\.\\d+)*$',  # Just numbers\n","                r'^(Contents|Previous|Next|Index|Chapter|Section)',\n","                r'^Oracle and/or',\n","                r'^Copyright',\n","                r'^All rights reserved'\n","            ]\n","\n","            if any(re.match(pattern, sent) for pattern in skip_patterns):\n","                continue\n","\n","            # Filter reasonable length sentences\n","            if 15 <= len(sent) <= 800:\n","                cleaned_sentences.append(sent)\n","\n","        return cleaned_sentences\n","\n","    def is_rule_sentence(self, sentence: str) -> bool:\n","        \"\"\"Rule detection with better pattern matching\"\"\"\n","        # Check for rule indicators\n","        has_rule_pattern = any(re.search(pattern, sentence, re.IGNORECASE) for pattern in self.rule_patterns)\n","\n","        # Additional checks for specification language\n","        spec_indicators = [\n","            r'\\b(specification|standard|requirement)\\b',\n","            r'\\b(behavior|behaviour|semantics)\\b',\n","            r'\\b(valid|invalid|legal|illegal)\\b',\n","            r'\\b(throws?|catch|exception handling)\\b'\n","        ]\n","\n","        has_spec_language = any(re.search(pattern, sentence, re.IGNORECASE) for pattern in spec_indicators)\n","\n","        # Avoid sentences that are just examples or notes\n","        avoid_patterns = [\n","            r'^(For example|Note that|See also|Example)',\n","            r'^(The following|As shown|Consider)',\n","            r'^\\d+\\.\\d+',  # Section numbers\n","        ]\n","\n","        has_avoid_pattern = any(re.match(pattern, sentence, re.IGNORECASE) for pattern in avoid_patterns)\n","\n","        return (has_rule_pattern or has_spec_language) and not has_avoid_pattern\n","\n","    def extract_rules(self, section_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n","        \"\"\"Extract rule-like sentences with version and section metadata\"\"\"\n","        text = section_data.get('text', '')\n","        metadata = section_data.get('metadata', {})\n","\n","        sentences = self.extract_sentences(text)\n","        rules = []\n","\n","        for sent in sentences:\n","            if self.is_rule_sentence(sent):\n","                # Process with spaCy\n","                doc = self.nlp(sent)\n","\n","                # Extract linguistic features\n","                entities = [(ent.text, ent.label_) for ent in doc.ents]\n","                dependencies = [(token.text, token.dep_, token.head.text) for token in doc]\n","\n","                # Identify modal verbs and their context\n","                modals = []\n","                for token in doc:\n","                    if token.text.lower() in ['must', 'shall', 'should', 'may', 'cannot', 'will']:\n","                        context_start = max(0, token.i - 3)\n","                        context_end = min(len(doc), token.i + 4)\n","                        context = ' '.join([t.text for t in doc[context_start:context_end]])\n","\n","                        modals.append({\n","                            'modal': token.text.lower(),\n","                            'context': context\n","                        })\n","\n","                # Create rule with metadata\n","                rule = {\n","                    'text': sent,\n","                    'java_version': metadata.get('java_version', 'unknown'),\n","                    'section_number': metadata.get('section_number', 'unknown'),\n","                    'section_title': metadata.get('section_title', 'unknown'),\n","                    'chapter_title': metadata.get('chapter_title', 'unknown'),\n","                    'section_name': section_data.get('section_name', 'unknown'),\n","                    'source_url': section_data.get('url', ''),\n","                    'extracted_at': datetime.now().isoformat(),\n","                    'entities': entities,\n","                    'dependencies': dependencies,\n","                    'modals': modals,\n","                    'tokens': [token.text for token in doc],\n","                    'pos_tags': [(token.text, token.pos_) for token in doc]\n","                }\n","\n","                rules.append(rule)\n","\n","        return rules\n","\n","# =============================================================================\n","# PHASE 5: LLM-BASED RULE ANALYSIS WITH BETTER PROMPTS\n","# =============================================================================\n","\n","class EnhancedLLMRuleAnalyzer:\n","    def __init__(self, llm_client):\n","        self.llm_client = llm_client\n","        self.categories = [\n","            \"METHOD_RESOLUTION\",\n","            \"TYPE_COMPATIBILITY\",\n","            \"INHERITANCE_RULES\",\n","            \"OVERLOADING_RULES\",\n","            \"ACCESS_CONTROL\",\n","            \"EXCEPTION_HANDLING\",\n","            \"GENERICS_RULES\",\n","            \"INTERFACE_RULES\",\n","            \"CONSTRUCTOR_RULES\",\n","            \"COMPILATION_RULES\"\n","        ]\n","\n","    def categorize_rule(self, rule_text: str, java_version: str, section_info: str) -> str:\n","        \"\"\"Rule categorization with better context\"\"\"\n","        prompt = f\"\"\"You are analyzing Java Language Specification rules. Categorize this rule into exactly ONE category.\n","\n","Java Version: {java_version}\n","Section Context: {section_info}\n","\n","Rule Text: \"{rule_text}\"\n","\n","Available Categories:\n","- METHOD_RESOLUTION: Rules about how method calls are resolved\n","- TYPE_COMPATIBILITY: Rules about type assignments and conversions\n","- INHERITANCE_RULES: Rules about class inheritance and method overriding\n","- OVERLOADING_RULES: Rules about method overloading resolution\n","- ACCESS_CONTROL: Rules about visibility and access modifiers\n","- EXCEPTION_HANDLING: Rules about throwing, catching, and declaring exceptions\n","- GENERICS_RULES: Rules about generic types and type parameters\n","- INTERFACE_RULES: Rules about interface declarations and implementations\n","- CONSTRUCTOR_RULES: Rules about constructor declarations and invocation\n","- COMPILATION_RULES: Rules about compile-time checking and errors\n","\n","Choose the MOST SPECIFIC category that applies. Return only the category name.\"\"\"\n","\n","        try:\n","            response = self.llm_client.call_llm(prompt, max_tokens=50, temperature=0.1)\n","            category = response.strip().upper().replace(' ', '_')\n","            return category if category in self.categories else \"COMPILATION_RULES\"\n","        except Exception as e:\n","            print(f\"Error categorizing rule: {e}\")\n","            return \"COMPILATION_RULES\"\n","\n","    def extract_rule_components(self, rule_text: str, java_version: str) -> Dict[str, Any]:\n","        \"\"\"Component extraction with structured analysis\"\"\"\n","        prompt = f\"\"\"Analyze this Java {java_version} specification rule and extract its key components.\n","\n","Rule: \"{rule_text}\"\n","\n","Extract the following components and return ONLY valid JSON:\n","\n","{{\n","    \"subject\": \"What this rule applies to (e.g., 'method call', 'class declaration', 'type parameter')\",\n","    \"action\": \"The main action/relationship (e.g., 'overrides', 'implements', 'resolves to', 'throws')\",\n","    \"modality\": \"Requirement strength (must/shall/should/may/cannot/prohibited)\",\n","    \"conditions\": [\"List of conditions that must be met\", \"for this rule to apply\"],\n","    \"consequences\": [\"What happens when rule applies\", \"expected outcomes or requirements\"],\n","    \"exceptions\": [\"Any stated exceptions to this rule\", \"special cases where rule doesn't apply\"],\n","    \"scope\": \"When this rule applies (compile-time/runtime/both)\"\n","}}\n","\n","Focus on extracting concrete, specific information. If a field is not clearly present, use an empty array [] or \"not specified\".\"\"\"\n","\n","        try:\n","            response = self.llm_client.call_llm(prompt, max_tokens=600, temperature=0.2)\n","            # Extract JSON from response\n","            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n","            if json_match:\n","                return json.loads(json_match.group())\n","            return {\n","                \"subject\": \"not specified\",\n","                \"action\": \"not specified\",\n","                \"modality\": \"not specified\",\n","                \"conditions\": [],\n","                \"consequences\": [],\n","                \"exceptions\": [],\n","                \"scope\": \"not specified\"\n","            }\n","        except Exception as e:\n","            print(f\"Error extracting components: {e}\")\n","            return {}\n","\n","    def convert_to_formal_logic(self, rule_text: str, components: Dict, java_version: str) -> str:\n","        \"\"\"Formal logic conversion with Java-specific predicates\"\"\"\n","        prompt = f\"\"\"Convert this Java {java_version} specification rule to formal first-order logic.\n","\n","Rule: \"{rule_text}\"\n","Components: {json.dumps(components, indent=2)}\n","\n","Use these Java-specific predicates:\n","- Method(m), Class(c), Interface(i), Type(t), Variable(v)\n","- Overrides(m1,m2), Implements(c,i), Extends(c1,c2)\n","- Accessible(x,context), Compatible(t1,t2), Assignable(t1,t2)\n","- Throws(m,exception), Declares(entity,property)\n","- CompileTime(condition), Runtime(condition)\n","- HasModifier(entity,modifier) where modifier âˆˆ {{public,private,protected,static,final,abstract}}\n","\n","Logical operators: âˆ€ (forall), âˆƒ (exists), â†’ (implies), âˆ§ (and), âˆ¨ (or), Â¬ (not)\n","\n","Express the rule as a clear logical statement. If the rule has conditions, use implications (â†’).\n","\n","Example format:\n","âˆ€m,c: Method(m) âˆ§ Declares(c,m) âˆ§ HasModifier(m,private) â†’ Â¬Accessible(m,Subclass(c))\n","\n","Return only the formal logic expression:\"\"\"\n","\n","        try:\n","            response = self.llm_client.call_llm(prompt, max_tokens=300, temperature=0.1)\n","            return response.strip()\n","        except Exception as e:\n","            print(f\"Error converting to formal logic: {e}\")\n","            return \"Logic conversion failed\"\n","\n","# =============================================================================\n","# PHASE 6: MODE-AWARE CONFLICT DETECTION ENGINE\n","# =============================================================================\n","\n","class EnhancedConflictDetector:\n","    def __init__(self, llm_client, mode='full'):\n","        self.llm_client = llm_client\n","        self.mode = mode\n","        self.rules_db = []\n","        self.potential_conflicts = []\n","        self.conflict_types = [\n","            \"CONTRADICTION\",  # Rules that directly contradict each other\n","            \"AMBIGUITY\",      # Rules that create unclear situations\n","            \"OVERLAP\",        # Rules that apply to same situation differently\n","            \"VERSION_CHANGE\", # Rules that changed between Java versions\n","            \"SCOPE_CONFLICT\", # Rules with overlapping but different scopes\n","            \"PRECEDENCE\"      # Rules with unclear precedence order\n","        ]\n","\n","    def add_rule(self, rule_data: Dict[str, Any]):\n","        \"\"\"Add processed rule to database with indexing\"\"\"\n","        rule_id = f\"rule_{len(self.rules_db)}_{rule_data.get('java_version', 'unknown')}\"\n","\n","        enhanced_rule = {\n","            'id': rule_id,\n","            'text': rule_data.get('text', ''),\n","            'java_version': rule_data.get('java_version', 'unknown'),\n","            'section_number': rule_data.get('section_number', 'unknown'),\n","            'section_name': rule_data.get('section_name', 'unknown'),\n","            'category': rule_data.get('category', 'UNKNOWN'),\n","            'components': rule_data.get('components', {}),\n","            'formal_logic': rule_data.get('formal_logic', ''),\n","            'keywords': self._extract_keywords(rule_data.get('text', '')),\n","            'modality': rule_data.get('components', {}).get('modality', 'not specified'),\n","            'scope': rule_data.get('components', {}).get('scope', 'not specified'),\n","            'added_at': datetime.now().isoformat()\n","        }\n","\n","        self.rules_db.append(enhanced_rule)\n","        return rule_id\n","\n","    def _extract_keywords(self, text: str) -> List[str]:\n","        \"\"\"Extract key terms from rule text for matching\"\"\"\n","        # Common Java language specification keywords\n","        java_keywords = [\n","            'method', 'class', 'interface', 'type', 'variable', 'field',\n","            'constructor', 'inheritance', 'override', 'overload', 'implement',\n","            'extend', 'abstract', 'final', 'static', 'private', 'protected',\n","            'public', 'package', 'generic', 'parameter', 'argument', 'return',\n","            'exception', 'throw', 'catch', 'compile', 'runtime', 'accessible',\n","            'visible', 'compatible', 'assignable', 'resolution', 'invocation'\n","        ]\n","\n","        text_lower = text.lower()\n","        found_keywords = []\n","\n","        for keyword in java_keywords:\n","            if keyword in text_lower:\n","                found_keywords.append(keyword)\n","\n","        # Also extract quoted terms and technical terms\n","        quoted_terms = re.findall(r'\"([^\"]*)\"', text)\n","        technical_terms = re.findall(r'\\b[A-Z][a-zA-Z]*(?:[A-Z][a-zA-Z]*)*\\b', text)\n","\n","        found_keywords.extend(quoted_terms)\n","        found_keywords.extend(technical_terms)\n","\n","        return list(set(found_keywords))  # Remove duplicates\n","\n","    def find_potential_conflicts(self) -> List[Dict[str, Any]]:\n","        \"\"\"Find potential conflicts between rules with mode-aware analysis\"\"\"\n","        conflicts = []\n","\n","        # In test mode, limit comparisons to reduce processing time\n","        if self.mode == 'test':\n","            max_comparisons = 50\n","            print(f\"ðŸ§ª Test mode: Limiting to {max_comparisons} rule comparisons\")\n","        else:\n","            max_comparisons = len(self.rules_db) * (len(self.rules_db) - 1) // 2\n","            print(f\"ðŸš€ Full mode: Analyzing {max_comparisons} rule combinations\")\n","\n","        comparisons_made = 0\n","\n","        for i, rule1 in enumerate(self.rules_db):\n","            for j, rule2 in enumerate(self.rules_db[i+1:], i+1):\n","                if comparisons_made >= max_comparisons:\n","                    break\n","\n","                # Quick pre-filtering\n","                if self._should_compare_rules(rule1, rule2):\n","                    conflict = self._analyze_rule_pair(rule1, rule2)\n","                    if conflict:\n","                        conflicts.append(conflict)\n","                        print(f\"   ðŸ” Potential conflict found: {conflict['type']} between Java {rule1['java_version']} and {rule2['java_version']}\")\n","\n","                comparisons_made += 1\n","\n","                # Progress indicator for full mode\n","                if self.mode == 'full' and comparisons_made % 100 == 0:\n","                    print(f\"   ðŸ“Š Analyzed {comparisons_made}/{max_comparisons} combinations...\")\n","\n","            if comparisons_made >= max_comparisons:\n","                break\n","\n","        self.potential_conflicts = conflicts\n","        print(f\"âœ… Found {len(conflicts)} potential conflicts\")\n","        return conflicts\n","\n","    def _should_compare_rules(self, rule1: Dict, rule2: Dict) -> bool:\n","        \"\"\"Quick filtering to determine if two rules should be compared\"\"\"\n","        # Don't compare identical rules\n","        if rule1['id'] == rule2['id']:\n","            return False\n","\n","        # Don't compare if they have no overlapping keywords\n","        common_keywords = set(rule1['keywords']) & set(rule2['keywords'])\n","        if len(common_keywords) == 0:\n","            return False\n","\n","        # Always compare rules from different Java versions with same category\n","        if (rule1['java_version'] != rule2['java_version'] and\n","            rule1['category'] == rule2['category']):\n","            return True\n","\n","        # Compare rules with similar scope or modality\n","        if (rule1['scope'] == rule2['scope'] or\n","            rule1['modality'] == rule2['modality']):\n","            return True\n","\n","        # Compare rules with significant keyword overlap\n","        overlap_ratio = len(common_keywords) / max(len(rule1['keywords']), len(rule2['keywords']), 1)\n","        return overlap_ratio > 0.3\n","\n","    def _analyze_rule_pair(self, rule1: Dict, rule2: Dict) -> Optional[Dict[str, Any]]:\n","        \"\"\"Analyze a pair of rules for potential conflicts using LLM\"\"\"\n","        try:\n","            conflict_analysis_prompt = f\"\"\"Analyze these two Java Language Specification rules for potential conflicts.\n","\n","RULE 1 (Java {rule1['java_version']}, Section {rule1['section_number']}):\n","\"{rule1['text']}\"\n","Category: {rule1['category']}\n","Modality: {rule1['modality']}\n","Scope: {rule1['scope']}\n","\n","RULE 2 (Java {rule2['java_version']}, Section {rule2['section_number']}):\n","\"{rule2['text']}\"\n","Category: {rule2['category']}\n","Modality: {rule2['modality']}\n","Scope: {rule2['scope']}\n","\n","Analyze for these conflict types:\n","1. CONTRADICTION - Rules directly contradict each other\n","2. AMBIGUITY - Rules create unclear or ambiguous situations\n","3. OVERLAP - Rules apply to same situation with different requirements\n","4. VERSION_CHANGE - Rule changed between Java versions\n","5. SCOPE_CONFLICT - Overlapping but different scopes\n","6. PRECEDENCE - Unclear which rule takes precedence\n","\n","Return ONLY this JSON format:\n","{{\n","    \"has_conflict\": true/false,\n","    \"conflict_type\": \"TYPE_FROM_ABOVE_OR_NONE\",\n","    \"severity\": \"LOW/MEDIUM/HIGH\",\n","    \"description\": \"Detailed explanation of the conflict\",\n","    \"affected_scenarios\": [\"Specific scenarios where conflict occurs\"],\n","    \"resolution_needed\": \"What needs clarification\"\n","}}\n","\n","Be precise and only identify genuine conflicts, not minor differences.\"\"\"\n","\n","            response = self.llm_client.call_llm(\n","                conflict_analysis_prompt,\n","                max_tokens=500,\n","                temperature=0.1\n","            )\n","\n","            # Extract JSON from response\n","            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n","            if json_match:\n","                analysis = json.loads(json_match.group())\n","\n","                if analysis.get('has_conflict', False):\n","                    return {\n","                        'rule1_id': rule1['id'],\n","                        'rule2_id': rule2['id'],\n","                        'rule1_version': rule1['java_version'],\n","                        'rule2_version': rule2['java_version'],\n","                        'rule1_section': rule1['section_number'],\n","                        'rule2_section': rule2['section_number'],\n","                        'type': analysis.get('conflict_type', 'UNKNOWN'),\n","                        'severity': analysis.get('severity', 'MEDIUM'),\n","                        'description': analysis.get('description', ''),\n","                        'affected_scenarios': analysis.get('affected_scenarios', []),\n","                        'resolution_needed': analysis.get('resolution_needed', ''),\n","                        'detected_at': datetime.now().isoformat(),\n","                        'rule1_text': rule1['text'][:200] + '...',\n","                        'rule2_text': rule2['text'][:200] + '...'\n","                    }\n","\n","            return None\n","\n","        except Exception as e:\n","            print(f\"   âš ï¸ Error analyzing rule pair: {e}\")\n","            return None\n","\n","    def generate_conflict_report(self) -> Dict[str, Any]:\n","        \"\"\"Generate comprehensive conflict analysis report\"\"\"\n","        if not self.potential_conflicts:\n","            return {\n","                'summary': 'No conflicts detected',\n","                'total_conflicts': 0,\n","                'by_type': {},\n","                'by_severity': {},\n","                'by_version_pair': {},\n","                'recommendations': []\n","            }\n","\n","        # Analyze conflicts by type\n","        by_type = {}\n","        by_severity = {}\n","        by_version_pair = {}\n","\n","        for conflict in self.potential_conflicts:\n","            # By type\n","            conflict_type = conflict.get('type', 'UNKNOWN')\n","            by_type[conflict_type] = by_type.get(conflict_type, 0) + 1\n","\n","            # By severity\n","            severity = conflict.get('severity', 'MEDIUM')\n","            by_severity[severity] = by_severity.get(severity, 0) + 1\n","\n","            # By version pair\n","            v1, v2 = conflict['rule1_version'], conflict['rule2_version']\n","            version_pair = f\"Java {v1} vs Java {v2}\"\n","            by_version_pair[version_pair] = by_version_pair.get(version_pair, 0) + 1\n","\n","        # Generate recommendations\n","        recommendations = self._generate_recommendations(by_type, by_severity, by_version_pair)\n","\n","        report = {\n","            'total_conflicts': len(self.potential_conflicts),\n","            'by_type': by_type,\n","            'by_severity': by_severity,\n","            'by_version_pair': by_version_pair,\n","            'recommendations': recommendations,\n","            'detailed_conflicts': self.potential_conflicts[:10],  # Top 10 conflicts\n","            'analysis_mode': self.mode,\n","            'rules_analyzed': len(self.rules_db),\n","            'generated_at': datetime.now().isoformat()\n","        }\n","\n","        return report\n","\n","    def _generate_recommendations(self, by_type: Dict, by_severity: Dict, by_version_pair: Dict) -> List[str]:\n","        \"\"\"Generate actionable recommendations based on conflict analysis\"\"\"\n","        recommendations = []\n","\n","        # High severity recommendations\n","        if by_severity.get('HIGH', 0) > 0:\n","            recommendations.append(\n","                f\"ðŸš¨ URGENT: {by_severity['HIGH']} high-severity conflicts require immediate attention\"\n","            )\n","\n","        # Version-specific recommendations\n","        for version_pair, count in by_version_pair.items():\n","            if count > 3:  # Threshold for version conflict attention\n","                recommendations.append(\n","                    f\"ðŸ“‹ Review {version_pair} compatibility - {count} conflicts detected\"\n","                )\n","\n","        # Type-specific recommendations\n","        if by_type.get('CONTRADICTION', 0) > 0:\n","            recommendations.append(\n","                f\"âš ï¸ {by_type['CONTRADICTION']} direct contradictions need specification clarification\"\n","            )\n","\n","        if by_type.get('VERSION_CHANGE', 0) > 0:\n","            recommendations.append(\n","                f\"ðŸ”„ {by_type['VERSION_CHANGE']} version changes require migration guidance\"\n","            )\n","\n","        # General recommendations\n","        if len(by_type) > 3:\n","            recommendations.append(\n","                \"ðŸ“š Consider creating a unified specification guide to address multiple conflict types\"\n","            )\n","\n","        return recommendations\n","\n","# =============================================================================\n","# PHASE 7: EXECUTION ORCHESTRATOR\n","# =============================================================================\n","\n","class SpecSentinelOrchestrator:\n","    def __init__(self, mode='full', custom_section=None):\n","        self.mode = mode\n","        self.custom_section = custom_section\n","        self.project_path = None\n","        self.results = {}\n","\n","    def run_complete_analysis(self):\n","        \"\"\"Run the complete SpecSentinel analysis pipeline\"\"\"\n","        start_time = datetime.now()\n","\n","        try:\n","            # Phase 1: Setup\n","            print(f\"\\n{'='*60}\")\n","            print(f\"ðŸš€ STARTING SPECSENTINEL ANALYSIS ({self.mode.upper()} MODE)\")\n","            print(f\"{'='*60}\")\n","\n","            self.project_path = setup_project_path(self.mode)\n","\n","            # Phase 2: Initialize LLM client\n","            print(f\"\\nðŸ“¡ Phase 2: Initializing LLM Client...\")\n","            llm_client = EnhancedOpenRouterClient(self.mode)\n","\n","            # Phase 3: Download specifications\n","            print(f\"\\nðŸ“š Phase 3: Downloading Java Specifications...\")\n","            downloader = MultiVersionSpecificationDownloader(self.mode, self.custom_section)\n","            specifications = downloader.download_all_sections()\n","\n","            if not specifications:\n","                raise Exception(\"Failed to download specifications\")\n","\n","            # Save specifications\n","            with open(f'{self.project_path}/data/all_specifications.json', 'w') as f:\n","                json.dump(specifications, f, indent=2, ensure_ascii=False)\n","\n","            # Phase 4: Extract rules\n","            print(f\"\\nðŸ” Phase 4: Extracting Rules from Specifications...\")\n","            preprocessor = EnhancedSpecPreprocessor()\n","            all_rules = []\n","\n","            for spec_key, spec_data in specifications.items():\n","                print(f\"   Processing {spec_key}...\")\n","                rules = preprocessor.extract_rules(spec_data)\n","                all_rules.extend(rules)\n","                print(f\"   Extracted {len(rules)} rules\")\n","\n","            print(f\"âœ… Total rules extracted: {len(all_rules)}\")\n","\n","            # Save raw rules\n","            with open(f'{self.project_path}/data/extracted_rules.json', 'w') as f:\n","                json.dump(all_rules, f, indent=2, ensure_ascii=False)\n","\n","            # Phase 5: Analyze rules with LLM\n","            print(f\"\\nðŸ¤– Phase 5: Analyzing Rules with LLM...\")\n","            analyzer = EnhancedLLMRuleAnalyzer(llm_client)\n","            processed_rules = []\n","\n","            # Limit rule processing in test mode\n","            rules_to_process = all_rules[:20] if self.mode == 'test' else all_rules\n","            print(f\"Processing {len(rules_to_process)} rules...\")\n","\n","            for i, rule in enumerate(rules_to_process):\n","                try:\n","                    print(f\"   Analyzing rule {i+1}/{len(rules_to_process)}...\")\n","\n","                    # Categorize rule\n","                    section_info = f\"{rule['section_title']} ({rule['section_number']})\"\n","                    category = analyzer.categorize_rule(\n","                        rule['text'],\n","                        rule['java_version'],\n","                        section_info\n","                    )\n","\n","                    # Extract components\n","                    components = analyzer.extract_rule_components(\n","                        rule['text'],\n","                        rule['java_version']\n","                    )\n","\n","                    # Convert to formal logic\n","                    formal_logic = analyzer.convert_to_formal_logic(\n","                        rule['text'],\n","                        components,\n","                        rule['java_version']\n","                    )\n","\n","                    # Create processed rule\n","                    processed_rule = {\n","                        **rule,\n","                        'category': category,\n","                        'components': components,\n","                        'formal_logic': formal_logic,\n","                        'processed_at': datetime.now().isoformat()\n","                    }\n","\n","                    processed_rules.append(processed_rule)\n","\n","                    # Delay between API calls\n","                    time.sleep(1 if self.mode == 'test' else 0.5)\n","\n","                except Exception as e:\n","                    print(f\"   âš ï¸ Error processing rule {i+1}: {e}\")\n","                    continue\n","\n","            print(f\"âœ… Processed {len(processed_rules)} rules\")\n","\n","            # Save processed rules\n","            with open(f'{self.project_path}/data/processed_rules.json', 'w') as f:\n","                json.dump(processed_rules, f, indent=2, ensure_ascii=False)\n","\n","            # Phase 6: Detect conflicts\n","            print(f\"\\nðŸ” Phase 6: Detecting Conflicts...\")\n","            conflict_detector = EnhancedConflictDetector(llm_client, self.mode)\n","\n","            # Add all processed rules to conflict detector\n","            for rule in processed_rules:\n","                conflict_detector.add_rule(rule)\n","\n","            # Find conflicts\n","            conflicts = conflict_detector.find_potential_conflicts()\n","\n","            # Generate conflict report\n","            conflict_report = conflict_detector.generate_conflict_report()\n","\n","            # Save conflict report\n","            with open(f'{self.project_path}/results/conflict_report.json', 'w') as f:\n","                json.dump(conflict_report, f, indent=2, ensure_ascii=False)\n","\n","            # Phase 7: Generate final summary\n","            print(f\"\\nðŸ“Š Phase 7: Generating Summary Report...\")\n","            summary = self._generate_final_summary(\n","                specifications, all_rules, processed_rules, conflict_report, start_time\n","            )\n","\n","            # Save summary\n","            with open(f'{self.project_path}/results/final_summary.json', 'w') as f:\n","                json.dump(summary, f, indent=2, ensure_ascii=False)\n","\n","            # Display results\n","            self._display_results(summary, conflict_report)\n","\n","            return summary\n","\n","        except Exception as e:\n","            print(f\"âŒ Analysis failed: {e}\")\n","            return None\n","\n","    def _generate_final_summary(self, specifications, raw_rules, processed_rules, conflict_report, start_time):\n","        \"\"\"Generate comprehensive final summary\"\"\"\n","        end_time = datetime.now()\n","        duration = end_time - start_time\n","\n","        # Analyze rule distribution by category\n","        category_distribution = {}\n","        for rule in processed_rules:\n","            category = rule.get('category', 'UNKNOWN')\n","            category_distribution[category] = category_distribution.get(category, 0) + 1\n","\n","        # Analyze rules by Java version\n","        version_distribution = {}\n","        for rule in processed_rules:\n","            version = rule.get('java_version', 'unknown')\n","            version_distribution[version] = version_distribution.get(version, 0) + 1\n","\n","        summary = {\n","            'analysis_info': {\n","                'mode': self.mode,\n","                'start_time': start_time.isoformat(),\n","                'end_time': end_time.isoformat(),\n","                'duration_seconds': duration.total_seconds(),\n","                'duration_formatted': str(duration)\n","            },\n","            'data_statistics': {\n","                'specifications_downloaded': len(specifications),\n","                'raw_rules_extracted': len(raw_rules),\n","                'rules_processed': len(processed_rules),\n","                'processing_success_rate': f\"{len(processed_rules)/max(len(raw_rules), 1)*100:.1f}%\"\n","            },\n","            'rule_analysis': {\n","                'category_distribution': category_distribution,\n","                'version_distribution': version_distribution,\n","                'top_categories': sorted(category_distribution.items(), key=lambda x: x[1], reverse=True)[:5]\n","            },\n","            'conflict_analysis': {\n","                'total_conflicts': conflict_report.get('total_conflicts', 0),\n","                'by_severity': conflict_report.get('by_severity', {}),\n","                'by_type': conflict_report.get('by_type', {}),\n","                'critical_conflicts': [c for c in conflict_report.get('detailed_conflicts', [])\n","                                     if c.get('severity') == 'HIGH']\n","            },\n","            'recommendations': conflict_report.get('recommendations', []),\n","            'files_generated': [\n","                f'{self.project_path}/data/all_specifications.json',\n","                f'{self.project_path}/data/extracted_rules.json',\n","                f'{self.project_path}/data/processed_rules.json',\n","                f'{self.project_path}/results/conflict_report.json',\n","                f'{self.project_path}/results/final_summary.json'\n","            ]\n","        }\n","\n","        return summary\n","\n","    def _display_results(self, summary, conflict_report):\n","        \"\"\"Display final results in a formatted way\"\"\"\n","        print(f\"\\n{'='*80}\")\n","        print(f\"ðŸŽ¯ SPECSENTINEL ANALYSIS COMPLETE ({self.mode.upper()} MODE)\")\n","        print(f\"{'='*80}\")\n","\n","        # Analysis Statistics\n","        print(f\"\\nðŸ“Š Analysis Statistics:\")\n","        print(f\"   Duration: {summary['analysis_info']['duration_formatted']}\")\n","        print(f\"   Specifications: {summary['data_statistics']['specifications_downloaded']}\")\n","        print(f\"   Rules Extracted: {summary['data_statistics']['raw_rules_extracted']}\")\n","        print(f\"   Rules Processed: {summary['data_statistics']['rules_processed']}\")\n","        print(f\"   Success Rate: {summary['data_statistics']['processing_success_rate']}\")\n","\n","        # Rule Categories\n","        print(f\"\\nðŸ“š Top Rule Categories:\")\n","        for category, count in summary['rule_analysis']['top_categories']:\n","            print(f\"   {category}: {count} rules\")\n","\n","        # Conflict Summary\n","        print(f\"\\nðŸ” Conflict Analysis:\")\n","        print(f\"   Total Conflicts: {conflict_report['total_conflicts']}\")\n","\n","        if conflict_report['by_severity']:\n","            print(f\"   By Severity:\")\n","            for severity, count in conflict_report['by_severity'].items():\n","                print(f\"      {severity}: {count}\")\n","\n","        if conflict_report['by_type']:\n","            print(f\"   By Type:\")\n","            for conf_type, count in conflict_report['by_type'].items():\n","                print(f\"      {conf_type}: {count}\")\n","\n","        # Recommendations\n","        if conflict_report['recommendations']:\n","            print(f\"\\nðŸ’¡ Key Recommendations:\")\n","            for i, rec in enumerate(conflict_report['recommendations'][:5], 1):\n","                print(f\"   {i}. {rec}\")\n","\n","        # Files Generated\n","        print(f\"\\nðŸ“ Generated Files:\")\n","        for file_path in summary['files_generated']:\n","            if os.path.exists(file_path):\n","                size = os.path.getsize(file_path)\n","                print(f\"   âœ… {os.path.basename(file_path)} ({size:,} bytes)\")\n","            else:\n","                print(f\"   âŒ {os.path.basename(file_path)} (missing)\")\n","\n","        print(f\"\\nðŸŽ‰ Analysis complete! Results saved to: {self.project_path}\")\n","\n","    def export_for_agent(self):\n","        '''Export data in format suitable for the agent'''\n","        agent_data = {\n","            'specifications': self.specifications,\n","            'processed_rules': self.processed_rules,\n","            'conflicts': self.conflict_report,\n","            'summary': self.summary\n","        }\n","\n","        export_path = f'{self.project_path}/agent_knowledge_base.json'\n","        with open(export_path, 'w', encoding='utf-8') as f:\n","            json.dump(agent_data, f, indent=2, ensure_ascii=False)\n","\n","        print(f\"âœ… Agent knowledge base exported to: {export_path}\")\n","\n","# =============================================================================\n","# MAIN EXECUTION FUNCTION\n","# =============================================================================\n","\n","def main():\n","    \"\"\"Main execution function with mode selection\"\"\"\n","    # Phase 0: Mode selection\n","    mode = select_run_mode()\n","\n","    if mode is None:\n","        print(\"Analysis cancelled.\")\n","        return\n","\n","    shall_get_custom_section_choice = get_custom_section_choice()\n","\n","    if shall_get_custom_section_choice is None:\n","        print(\"Analysis cancelled.\")\n","        return\n","\n","    custom_section_details = None\n","\n","    if shall_get_custom_section_choice:\n","        custom_section_details = get_custom_section_details()\n","\n","    # Initialize and run orchestrator\n","    orchestrator = SpecSentinelOrchestrator(mode, custom_section_details)\n","    # Store specifications, processed_rules, conflicts, and summary as attributes\n","    # so they can be accessed by export_for_agent\n","    orchestrator.run_complete_analysis()\n","\n","    # Access the results directly from the orchestrator instance\n","    analysis_results = orchestrator.results\n","\n","    if analysis_results:\n","        print(f\"\\nâœ… SpecSentinel analysis completed successfully in {mode} mode!\")\n","        # Export for agent\n","        orchestrator.export_for_agent()\n","\n","        # Decide which interface to run based on user input or config\n","        # If you want to launch Streamlit automatically after analysis:\n","        print(\"\\nðŸš€ Launching Streamlit Interface...\")\n","        # Assuming streamlit_app.py is saved in the project_path\n","        streamlit_script_path = f'{orchestrator.project_path}/streamlit_app.py'\n","        # Use nohup and & to run in the background\n","        # Use npx localtunnel to expose the port\n","        !nohup streamlit run {streamlit_script_path} & npx localtunnel --port 8501 &\n","\n","        # If you wanted the Colab text interface instead, you would call:\n","        # run_colab_interface()\n","\n","    else:\n","        print(f\"\\nâŒ SpecSentinel analysis failed.\")\n","\n","# Ensure the main function is called\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","source":["# =============================================================================\n","# JAVA SPECIFICATION LLM AGENT WITH RAG SYSTEM\n","# =============================================================================\n","\n","import os\n","import json\n","import time\n","import pickle\n","import numpy as np\n","from datetime import datetime\n","from typing import List, Dict, Any, Optional, Tuple\n","import pandas as pd\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import streamlit as st\n","import requests\n","import re\n","from dataclasses import dataclass\n","from google.colab import drive\n","\n","# =============================================================================\n","# CONFIGURATION AND SETUP\n","# =============================================================================\n","\n","@dataclass\n","class AgentConfig:\n","    \"\"\"Configuration for the Java Spec Agent\"\"\"\n","    project_path: str = '/content/drive/My Drive/SpecSentinel'\n","    embedding_model: str = 'all-MiniLM-L6-v2'\n","    chunk_size: int = 512\n","    chunk_overlap: int = 50\n","    max_contexts: int = 5\n","    similarity_threshold: float = 0.3\n","    max_tokens_per_call: int = 1000\n","    temperature: float = 0.7\n","\n","class JavaSpecAgent:\n","    def __init__(self, config: AgentConfig, openrouter_api_key: str):\n","        self.config = config\n","        self.api_key = openrouter_api_key\n","        self.embedding_model = None\n","        self.knowledge_base = {}\n","        self.embeddings_cache = {}\n","        self.conversation_history = []\n","\n","        # Initialize components\n","        self._setup_environment()\n","        self._load_embedding_model()\n","        self._load_knowledge_base()\n","        self._build_vector_index()\n","\n","    def _setup_environment(self):\n","        \"\"\"Setup Google Drive and project environment\"\"\"\n","        try:\n","            drive.mount('/content/drive')\n","            print(\"âœ… Google Drive mounted successfully\")\n","        except:\n","            print(\"âš ï¸ Google Drive already mounted or unavailable\")\n","\n","        if not os.path.exists(self.config.project_path):\n","            os.makedirs(self.config.project_path, exist_ok=True)\n","            print(f\"âœ… Created project path: {self.config.project_path}\")\n","\n","    def _load_embedding_model(self):\n","        \"\"\"Load sentence transformer model for embeddings\"\"\"\n","        print(\"ðŸ“¦ Loading embedding model...\")\n","        self.embedding_model = SentenceTransformer(self.config.embedding_model)\n","        print(\"âœ… Embedding model loaded\")\n","\n","    def _load_knowledge_base(self):\n","        \"\"\"Load existing knowledge base from SpecSentinel results\"\"\"\n","        kb_files = {\n","            'specifications': f'{self.config.project_path}/data/all_specifications.json',\n","            'processed_rules': f'{self.config.project_path}/data/processed_rules.json',\n","            'conflicts': f'{self.config.project_path}/results/conflict_report.json',\n","            'summary': f'{self.config.project_path}/results/final_summary.json'\n","        }\n","\n","        print(\"ðŸ“š Loading knowledge base...\")\n","        for key, file_path in kb_files.items():\n","            if os.path.exists(file_path):\n","                with open(file_path, 'r', encoding='utf-8') as f:\n","                    self.knowledge_base[key] = json.load(f)\n","                print(f\"   âœ… Loaded {key}\")\n","            else:\n","                self.knowledge_base[key] = {}\n","                print(f\"   âš ï¸ {key} file not found\")\n","\n","        print(f\"âœ… Knowledge base loaded with {len(self.knowledge_base)} components\")\n","\n","    def _build_vector_index(self):\n","        \"\"\"Build vector index for RAG retrieval\"\"\"\n","        print(\"ðŸ” Building vector index...\")\n","\n","        # Check for cached embeddings\n","        embeddings_file = f'{self.config.project_path}/embeddings_cache.pkl'\n","\n","        if os.path.exists(embeddings_file):\n","            print(\"   Loading cached embeddings...\")\n","            with open(embeddings_file, 'rb') as f:\n","                self.embeddings_cache = pickle.load(f)\n","            print(\"   âœ… Cached embeddings loaded\")\n","            return\n","\n","        # Build new embeddings\n","        documents = self._prepare_documents()\n","\n","        if not documents:\n","            print(\"   âš ï¸ No documents to index\")\n","            return\n","\n","        # Create embeddings\n","        texts = [doc['text'] for doc in documents]\n","        embeddings = self.embedding_model.encode(texts, show_progress_bar=True)\n","\n","        # Cache embeddings with metadata\n","        self.embeddings_cache = {\n","            'embeddings': embeddings,\n","            'documents': documents,\n","            'created_at': datetime.now().isoformat()\n","        }\n","\n","        # Save cache\n","        with open(embeddings_file, 'wb') as f:\n","            pickle.dump(self.embeddings_cache, f)\n","\n","        print(f\"âœ… Vector index built with {len(documents)} documents\")\n","\n","    def _prepare_documents(self) -> List[Dict[str, Any]]:\n","        \"\"\"Prepare documents for vector indexing\"\"\"\n","        documents = []\n","\n","        # Process rules\n","        if 'processed_rules' in self.knowledge_base:\n","            for rule in self.knowledge_base['processed_rules']:\n","                doc = {\n","                    'id': f\"rule_{len(documents)}\",\n","                    'text': rule.get('text', ''),\n","                    'type': 'rule',\n","                    'java_version': rule.get('java_version', 'unknown'),\n","                    'section': rule.get('section_number', 'unknown'),\n","                    'category': rule.get('category', 'unknown'),\n","                    'metadata': rule\n","                }\n","                documents.append(doc)\n","\n","        # Process conflicts\n","        if 'conflicts' in self.knowledge_base and 'detailed_conflicts' in self.knowledge_base['conflicts']:\n","            for conflict in self.knowledge_base['conflicts']['detailed_conflicts']:\n","                conflict_text = f\"\"\"\n","                CONFLICT: {conflict.get('type', 'Unknown')}\n","                Severity: {conflict.get('severity', 'Unknown')}\n","                Description: {conflict.get('description', '')}\n","                Affected Scenarios: {' '.join(conflict.get('affected_scenarios', []))}\n","                Resolution Needed: {conflict.get('resolution_needed', '')}\n","                \"\"\"\n","\n","                doc = {\n","                    'id': f\"conflict_{len(documents)}\",\n","                    'text': conflict_text.strip(),\n","                    'type': 'conflict',\n","                    'java_versions': [conflict.get('rule1_version'), conflict.get('rule2_version')],\n","                    'severity': conflict.get('severity', 'unknown'),\n","                    'metadata': conflict\n","                }\n","                documents.append(doc)\n","\n","        # Process specifications\n","        if 'specifications' in self.knowledge_base:\n","            for spec_key, spec_data in self.knowledge_base['specifications'].items():\n","                # Chunk large specifications\n","                text = spec_data.get('text', '')\n","                chunks = self._chunk_text(text)\n","\n","                for i, chunk in enumerate(chunks):\n","                    doc = {\n","                        'id': f\"spec_{spec_key}_{i}\",\n","                        'text': chunk,\n","                        'type': 'specification',\n","                        'spec_key': spec_key,\n","                        'java_version': spec_data.get('metadata', {}).get('java_version', 'unknown'),\n","                        'section': spec_data.get('metadata', {}).get('section_number', 'unknown'),\n","                        'metadata': spec_data\n","                    }\n","                    documents.append(doc)\n","\n","        return documents\n","\n","    def _chunk_text(self, text: str) -> List[str]:\n","        \"\"\"Split text into chunks for better retrieval\"\"\"\n","        words = text.split()\n","        chunks = []\n","\n","        for i in range(0, len(words), self.config.chunk_size - self.config.chunk_overlap):\n","            chunk = ' '.join(words[i:i + self.config.chunk_size])\n","            if len(chunk.strip()) > 50:  # Only add substantial chunks\n","                chunks.append(chunk)\n","\n","        return chunks\n","\n","    def retrieve_relevant_context(self, query: str, max_results: int = None) -> List[Dict[str, Any]]:\n","        \"\"\"Retrieve relevant context using RAG\"\"\"\n","        if not self.embeddings_cache or 'embeddings' not in self.embeddings_cache:\n","            return []\n","\n","        max_results = max_results or self.config.max_contexts\n","\n","        # Encode query\n","        query_embedding = self.embedding_model.encode([query])\n","\n","        # Calculate similarities\n","        similarities = cosine_similarity(\n","            query_embedding,\n","            self.embeddings_cache['embeddings']\n","        )[0]\n","\n","        # Get top results above threshold\n","        relevant_indices = [\n","            i for i, sim in enumerate(similarities)\n","            if sim >= self.config.similarity_threshold\n","        ]\n","\n","        # Sort by similarity\n","        relevant_indices.sort(key=lambda i: similarities[i], reverse=True)\n","        relevant_indices = relevant_indices[:max_results]\n","\n","        # Return relevant documents with scores\n","        results = []\n","        for idx in relevant_indices:\n","            doc = self.embeddings_cache['documents'][idx].copy()\n","            doc['similarity_score'] = float(similarities[idx])\n","            results.append(doc)\n","\n","        return results\n","\n","    def _call_llm(self, prompt: str, max_tokens: int = None, temperature: float = None) -> str:\n","        \"\"\"Call OpenRouter API with efficient model selection\"\"\"\n","        max_tokens = max_tokens or self.config.max_tokens_per_call\n","        temperature = temperature or self.config.temperature\n","\n","        # Use cost-effective models\n","        models = [\n","            \"anthropic/claude-3.5-haiku-20241022\",  # Fast and cheap\n","            \"openai/gpt-4o-mini-2024-07-18\",       # Cost-effective\n","            \"deepseek/deepseek-chat-v3-0324:free\", # Free backup\n","            \"meta-llama/llama-3.2-3b-instruct:free\" # Free backup\n","        ]\n","\n","        for model in models:\n","            try:\n","                response = requests.post(\n","                    \"https://openrouter.ai/api/v1/chat/completions\",\n","                    headers={\n","                        \"Authorization\": f\"Bearer {self.api_key}\",\n","                        \"Content-Type\": \"application/json\",\n","                        \"HTTP-Referer\": \"https://github.com/java-spec-agent\",\n","                        \"X-Title\": \"Java Specification Agent\"\n","                    },\n","                    json={\n","                        \"model\": model,\n","                        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n","                        \"max_tokens\": max_tokens,\n","                        \"temperature\": temperature\n","                    },\n","                    timeout=30\n","                )\n","\n","                if response.status_code == 200:\n","                    result = response.json()\n","                    return result['choices'][0]['message']['content']\n","                elif response.status_code == 429:\n","                    time.sleep(2)  # Rate limit delay\n","                    continue\n","\n","            except Exception as e:\n","                print(f\"Model {model} failed: {e}\")\n","                continue\n","\n","        return \"I apologize, but I'm having trouble accessing my knowledge base right now. Please try again.\"\n","\n","    def classify_query_intent(self, query: str) -> str:\n","        \"\"\"Classify user query intent\"\"\"\n","        query_lower = query.lower()\n","\n","        # Greeting patterns\n","        greeting_patterns = ['hello', 'hi', 'hey', 'good morning', 'good afternoon', 'good evening']\n","        if any(pattern in query_lower for pattern in greeting_patterns):\n","            return 'greeting'\n","\n","        # Farewell patterns\n","        farewell_patterns = ['bye', 'goodbye', 'see you', 'farewell', 'exit', 'quit']\n","        if any(pattern in query_lower for pattern in farewell_patterns):\n","            return 'farewell'\n","\n","        # Conflict-related queries\n","        conflict_patterns = ['conflict', 'contradiction', 'ambiguity', 'inconsistency', 'problem', 'issue']\n","        if any(pattern in query_lower for pattern in conflict_patterns):\n","            return 'conflict_inquiry'\n","\n","        # Specification queries\n","        spec_patterns = ['specification', 'rule', 'section', 'jls', 'java language specification']\n","        if any(pattern in query_lower for pattern in spec_patterns):\n","            return 'specification_inquiry'\n","\n","        # Version comparison\n","        version_patterns = ['java 8', 'java 11', 'java 17', 'java 21', 'version', 'difference', 'change']\n","        if any(pattern in query_lower for pattern in version_patterns):\n","            return 'version_comparison'\n","\n","        return 'general_inquiry'\n","\n","    def generate_response(self, query: str) -> str:\n","        \"\"\"Generate response using RAG and LLM\"\"\"\n","        intent = self.classify_query_intent(query)\n","\n","        # Handle simple interactions without API calls\n","        if intent == 'greeting':\n","            return \"Hello! I'm your Java Language Specification assistant. I can help you with JLS rules, conflicts between versions, and specification details. What would you like to know?\"\n","\n","        if intent == 'farewell':\n","            return \"Goodbye! Feel free to come back anytime you have questions about Java specifications. Have a great day!\"\n","\n","        # For complex queries, use RAG\n","        relevant_docs = self.retrieve_relevant_context(query)\n","\n","        # Build context\n","        context_parts = []\n","        conflict_warnings = []\n","\n","        for doc in relevant_docs:\n","            if doc['type'] == 'conflict':\n","                conflict_warnings.append(f\"âš ï¸ CONFLICT ALERT: {doc['metadata'].get('description', '')}\")\n","                context_parts.append(f\"CONFLICT ({doc['metadata'].get('severity', 'Unknown')} severity): {doc['text']}\")\n","            elif doc['type'] == 'rule':\n","                context_parts.append(f\"RULE (Java {doc['java_version']}, Section {doc['section']}): {doc['text']}\")\n","            elif doc['type'] == 'specification':\n","                context_parts.append(f\"SPECIFICATION (Java {doc['java_version']}): {doc['text'][:300]}...\")\n","\n","        context = \"\\n\\n\".join(context_parts[:5])  # Limit context to manage token usage\n","\n","        # Build prompt based on intent\n","        if intent == 'conflict_inquiry':\n","            prompt = f\"\"\"You are a Java Language Specification expert. A user is asking about conflicts or issues in Java specifications.\n","\n","USER QUERY: {query}\n","\n","RELEVANT CONTEXT FROM KNOWLEDGE BASE:\n","{context}\n","\n","IMPORTANT: If there are conflicts in the specifications, you MUST warn the user about them and provide guidance on how to handle these conflicts.\n","\n","Provide a comprehensive answer that:\n","1. Directly addresses the user's question\n","2. Highlights any conflicts or ambiguities found\n","3. Provides practical guidance and recommendations\n","4. Mentions specific Java versions if relevant\n","5. Suggests best practices or workarounds\n","\n","Keep your response helpful, accurate, and focused on practical guidance.\"\"\"\n","\n","        else:\n","            prompt = f\"\"\"You are a helpful Java Language Specification assistant. Answer the user's question using the provided context from the official JLS.\n","\n","USER QUERY: {query}\n","\n","RELEVANT CONTEXT:\n","{context}\n","\n","Provide a clear, accurate answer that:\n","1. Directly addresses the user's question\n","2. Uses information from the context when relevant\n","3. Mentions specific Java versions or sections if applicable\n","4. Is helpful and easy to understand\n","5. Warns about any conflicts or issues if present\n","\n","If you don't have enough context to answer accurately, say so and suggest what specific information the user might need.\"\"\"\n","\n","        # Generate response\n","        response = self._call_llm(prompt, max_tokens=800)\n","\n","        # Add conflict warnings if found\n","        if conflict_warnings:\n","            response = \"\\n\\n\".join(conflict_warnings) + \"\\n\\n\" + response\n","\n","        return response\n","\n","    def add_to_knowledge_base(self, content: Dict[str, Any]) -> bool:\n","        \"\"\"Add new content to knowledge base (rules or conflicts)\"\"\"\n","        try:\n","            content_type = content.get('type', 'unknown')\n","\n","            if content_type == 'rule':\n","                # Add to processed rules\n","                if 'processed_rules' not in self.knowledge_base:\n","                    self.knowledge_base['processed_rules'] = []\n","\n","                self.knowledge_base['processed_rules'].append(content)\n","\n","                # Save updated rules\n","                rules_file = f'{self.config.project_path}/data/processed_rules.json'\n","                with open(rules_file, 'w', encoding='utf-8') as f:\n","                    json.dump(self.knowledge_base['processed_rules'], f, indent=2)\n","\n","            elif content_type == 'conflict':\n","                # Add to conflicts\n","                if 'conflicts' not in self.knowledge_base:\n","                    self.knowledge_base['conflicts'] = {'detailed_conflicts': []}\n","\n","                if 'detailed_conflicts' not in self.knowledge_base['conflicts']:\n","                    self.knowledge_base['conflicts']['detailed_conflicts'] = []\n","\n","                self.knowledge_base['conflicts']['detailed_conflicts'].append(content)\n","\n","                # Save updated conflicts\n","                conflicts_file = f'{self.config.project_path}/results/conflict_report.json'\n","                with open(conflicts_file, 'w', encoding='utf-8') as f:\n","                    json.dump(self.knowledge_base['conflicts'], f, indent=2)\n","\n","            # Rebuild vector index to include new content\n","            self._build_vector_index()\n","\n","            print(f\"âœ… Added new {content_type} to knowledge base\")\n","            return True\n","\n","        except Exception as e:\n","            print(f\"âŒ Error adding to knowledge base: {e}\")\n","            return False\n","\n","    def get_knowledge_stats(self) -> Dict[str, Any]:\n","        \"\"\"Get statistics about the knowledge base\"\"\"\n","        stats = {\n","            'total_rules': len(self.knowledge_base.get('processed_rules', [])),\n","            'total_conflicts': len(self.knowledge_base.get('conflicts', {}).get('detailed_conflicts', [])),\n","            'total_specifications': len(self.knowledge_base.get('specifications', {})),\n","            'java_versions': set(),\n","            'categories': set(),\n","            'last_updated': datetime.now().isoformat()\n","        }\n","\n","        # Collect versions and categories\n","        for rule in self.knowledge_base.get('processed_rules', []):\n","            stats['java_versions'].add(rule.get('java_version', 'unknown'))\n","            stats['categories'].add(rule.get('category', 'unknown'))\n","\n","        stats['java_versions'] = list(stats['java_versions'])\n","        stats['categories'] = list(stats['categories'])\n","\n","        return stats\n","\n","# =============================================================================\n","# STREAMLIT CHAT INTERFACE\n","# =============================================================================\n","\n","def create_streamlit_interface():\n","    \"\"\"Create Streamlit chat interface\"\"\"\n","    st.set_page_config(\n","        page_title=\"Java Specification Agent\",\n","        page_icon=\"â˜•\",\n","        layout=\"wide\"\n","    )\n","\n","    st.title(\"â˜• Java Language Specification Agent\")\n","    st.markdown(\"Your AI assistant for Java specifications, conflicts, and version differences\")\n","\n","    # Initialize session state\n","    if 'agent' not in st.session_state:\n","        api_key = userdata.get('OPENROUTER_API_KEY') or st.text_input(\"Enter OpenRouter API Key:\", type=\"password\")\n","        if api_key:\n","            config = AgentConfig()\n","            st.session_state.agent = JavaSpecAgent(config, api_key)\n","            st.session_state.messages = []\n","        else:\n","            st.warning(\"Please provide your OpenRouter API key to continue.\")\n","            return\n","\n","    # Sidebar with knowledge base stats\n","    with st.sidebar:\n","        st.header(\"ðŸ“Š Knowledge Base\")\n","        if hasattr(st.session_state, 'agent'):\n","            stats = st.session_state.agent.get_knowledge_stats()\n","            st.metric(\"Rules\", stats['total_rules'])\n","            st.metric(\"Conflicts\", stats['total_conflicts'])\n","            st.metric(\"Specifications\", stats['total_specifications'])\n","            st.write(\"**Java Versions:**\", \", \".join(stats['java_versions']))\n","\n","            # Add new content section\n","            st.header(\"âž• Add Content\")\n","            if st.button(\"Add New Rule\"):\n","                st.session_state.show_add_rule = True\n","            if st.button(\"Add New Conflict\"):\n","                st.session_state.show_add_conflict = True\n","\n","    # Main chat interface\n","    for message in st.session_state.messages:\n","        with st.chat_message(message[\"role\"]):\n","            st.markdown(message[\"content\"])\n","\n","    # Chat input\n","    if prompt := st.chat_input(\"Ask about Java specifications...\"):\n","        # Add user message\n","        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n","        with st.chat_message(\"user\"):\n","            st.markdown(prompt)\n","\n","        # Generate response\n","        with st.chat_message(\"assistant\"):\n","            with st.spinner(\"Thinking...\"):\n","                response = st.session_state.agent.generate_response(prompt)\n","                st.markdown(response)\n","                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n","\n","    # Add content forms\n","    if st.session_state.get('show_add_rule', False):\n","        with st.form(\"add_rule_form\"):\n","            st.subheader(\"Add New Rule\")\n","            rule_text = st.text_area(\"Rule Text\")\n","            java_version = st.selectbox(\"Java Version\", [\"8\", \"11\", \"17\", \"21\", \"24\"])\n","            section = st.text_input(\"Section Number\")\n","            category = st.selectbox(\"Category\", [\n","                \"METHOD_RESOLUTION\", \"TYPE_COMPATIBILITY\", \"INHERITANCE_RULES\",\n","                \"OVERLOADING_RULES\", \"ACCESS_CONTROL\", \"EXCEPTION_HANDLING\",\n","                \"GENERICS_RULES\", \"INTERFACE_RULES\", \"CONSTRUCTOR_RULES\", \"COMPILATION_RULES\"\n","            ])\n","\n","            if st.form_submit_button(\"Add Rule\"):\n","                new_rule = {\n","                    'type': 'rule',\n","                    'text': rule_text,\n","                    'java_version': java_version,\n","                    'section_number': section,\n","                    'category': category,\n","                    'added_at': datetime.now().isoformat()\n","                }\n","                if st.session_state.agent.add_to_knowledge_base(new_rule):\n","                    st.success(\"Rule added successfully!\")\n","                    st.session_state.show_add_rule = False\n","\n","# =============================================================================\n","# COLAB INTERFACE (Alternative to Streamlit)\n","# =============================================================================\n","\n","class ColabChatInterface:\n","    \"\"\"Simple chat interface for Google Colab\"\"\"\n","\n","    def __init__(self, agent: JavaSpecAgent):\n","        self.agent = agent\n","        self.conversation_history = []\n","\n","    def start_chat(self):\n","        \"\"\"Start interactive chat session\"\"\"\n","        print(\"â˜• Java Language Specification Agent\")\n","        print(\"=\"*50)\n","        print(\"Type 'quit' or 'exit' to end the conversation\")\n","        print(\"Type 'stats' to see knowledge base statistics\")\n","        print(\"Type 'help' for available commands\")\n","        print(\"=\"*50)\n","\n","        while True:\n","            try:\n","                user_input = input(\"\\nðŸ§‘ You: \").strip()\n","\n","                if user_input.lower() in ['quit', 'exit', 'bye']:\n","                    print(\"\\nðŸ¤– Agent: Goodbye! Thanks for using Java Spec Agent!\")\n","                    break\n","\n","                if user_input.lower() == 'stats':\n","                    self._show_stats()\n","                    continue\n","\n","                if user_input.lower() == 'help':\n","                    self._show_help()\n","                    continue\n","\n","                if user_input.lower() == 'clear':\n","                    self.conversation_history.clear()\n","                    print(\"\\nðŸ¤– Agent: Conversation history cleared!\")\n","                    continue\n","\n","                if not user_input:\n","                    continue\n","\n","                print(\"\\nðŸ¤– Agent: \", end=\"\", flush=True)\n","                response = self.agent.generate_response(user_input)\n","                print(response)\n","\n","                # Store conversation\n","                self.conversation_history.append({\n","                    'user': user_input,\n","                    'agent': response,\n","                    'timestamp': datetime.now().isoformat()\n","                })\n","\n","            except KeyboardInterrupt:\n","                print(\"\\n\\nðŸ¤– Agent: Goodbye!\")\n","                break\n","            except Exception as e:\n","                print(f\"\\nâŒ Error: {e}\")\n","\n","    def _show_stats(self):\n","        \"\"\"Show knowledge base statistics\"\"\"\n","        stats = self.agent.get_knowledge_stats()\n","        print(\"\\nðŸ“Š Knowledge Base Statistics:\")\n","        print(f\"   Rules: {stats['total_rules']}\")\n","        print(f\"   Conflicts: {stats['total_conflicts']}\")\n","        print(f\"   Specifications: {stats['total_specifications']}\")\n","        print(f\"   Java Versions: {', '.join(stats['java_versions'])}\")\n","        print(f\"   Categories: {', '.join(stats['categories'][:5])}{'...' if len(stats['categories']) > 5 else ''}\")\n","\n","    def _show_help(self):\n","        \"\"\"Show available commands\"\"\"\n","        print(\"\\nðŸ“‹ Available Commands:\")\n","        print(\"   'stats' - Show knowledge base statistics\")\n","        print(\"   'clear' - Clear conversation history\")\n","        print(\"   'help' - Show this help message\")\n","        print(\"   'quit' or 'exit' - End conversation\")\n","        print(\"\\nðŸ’¡ You can ask about:\")\n","        print(\"   - Java specification rules and sections\")\n","        print(\"   - Conflicts between Java versions\")\n","        print(\"   - Best practices and recommendations\")\n","        print(\"   - Version differences and migration guides\")\n","\n","# =============================================================================\n","# MAIN EXECUTION FUNCTIONS\n","# =============================================================================\n","\n","def run_colab_interface():\n","    \"\"\"Run the agent in Google Colab\"\"\"\n","    from google.colab import userdata\n","\n","    # Get API key\n","    try:\n","        api_key = userdata.get('OPENROUTER_API_KEY')\n","    except:\n","        from getpass import getpass\n","        api_key = getpass(\"Enter your OpenRouter API key: \")\n","\n","    # Initialize agent\n","    config = AgentConfig()\n","    agent = JavaSpecAgent(config, api_key)\n","\n","    # Start chat interface\n","    chat_interface = ColabChatInterface(agent)\n","    chat_interface.start_chat()\n","\n","def run_streamlit_interface():\n","    \"\"\"Run the Streamlit interface\"\"\"\n","    create_streamlit_interface()\n","\n","# Example usage and testing\n","def test_agent():\n","    \"\"\"Test the agent with sample queries\"\"\"\n","    from google.colab import userdata\n","\n","    api_key = userdata.get('OPENROUTER_API_KEY')\n","    config = AgentConfig()\n","    agent = JavaSpecAgent(config, api_key)\n","\n","    test_queries = [\n","        \"Hello!\",\n","        \"What are the conflicts in method resolution between Java 8 and Java 17?\",\n","        \"Tell me about switch statement rules\",\n","        \"Are there any ambiguities in generic type parameters?\",\n","        \"What changed in inheritance rules from Java 11 to Java 21?\",\n","        \"Goodbye!\"\n","    ]\n","\n","    print(\"ðŸ§ª Testing Java Spec Agent\")\n","    print(\"=\"*40)\n","\n","    for query in test_queries:\n","        print(f\"\\nðŸ§‘ Query: {query}\")\n","        print(\"ðŸ¤– Response:\", agent.generate_response(query))\n","        print(\"-\" * 40)\n","\n","# =============================================================================\n","# INTEGRATION INSTRUCTIONS\n","# =============================================================================\n","\n","\"\"\"\n","INTEGRATION WITH EXISTING CODE:\n","\n","1. Add these imports to your existing SpecSentinel code:\n","   - from sentence_transformers import SentenceTransformer\n","   - import streamlit as st (if using Streamlit)\n","\n","2. Modify the SpecSentinelOrchestrator class to export RAG data:\n","\n","   Add this method to your existing orchestrator:\n","\n","   def export_for_agent(self):\n","       '''Export data in format suitable for the agent'''\n","       agent_data = {\n","           'specifications': self.specifications,\n","           'processed_rules': self.processed_rules,\n","           'conflicts': self.conflict_report,\n","           'summary': self.summary\n","       }\n","\n","       export_path = f'{self.project_path}/agent_knowledge_base.json'\n","       with open(export_path, 'w', encoding='utf-8') as f:\n","           json.dump(agent_data, f, indent=2, ensure_ascii=False)\n","\n","       print(f\"âœ… Agent knowledge base exported to: {export_path}\")\n","\n","3. Run the agent after SpecSentinel analysis:\n","\n","   # After running your existing SpecSentinel code:\n","   orchestrator = SpecSentinelOrchestrator(mode, custom_section_details)\n","   results = orchestrator.run_complete_analysis()\n","\n","   if results:\n","       # Export for agent\n","       orchestrator.export_for_agent()\n","\n","       # Run the agent\n","       run_colab_interface()  # or run_streamlit_interface()\n","\n","DEPLOYMENT OPTIONS:\n","\n","1. Google Colab (Recommended):\n","   - Run run_colab_interface() directly in Colab\n","   - Uses your existing Google Drive storage\n","   - No additional setup required\n","\n","2. Streamlit (if available):\n","   - Install: !pip install streamlit\n","   - Run: run_streamlit_interface()\n","   - Access via web interface\n","\n","3. Jupyter Notebook:\n","   - Similar to Colab interface\n","   - Run run_colab_interface()\n","\n","COST OPTIMIZATION:\n","\n","The agent is designed to minimize API costs:\n","- Uses embeddings for RAG (runs locally)\n","- Prioritizes cheaper models\n","- Caches responses where possible\n","- Handles simple queries without API calls\n","- Efficient context management\n","\n","Your $10 OpenRouter credit should last for hundreds of interactions.\n","\"\"\"\n","\n","if __name__ == \"__main__\":\n","    # Uncomment the interface you want to use:\n","    run_colab_interface()      # For Google Colab\n","    # run_streamlit_interface()  # For Streamlit web interface\n","    # test_agent()              # For testing"],"metadata":{"id":"O7DasXBgxfbh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell to get the public IP address of the Colab instance\n","!curl ifconfig.me"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hK2cTHuYVZVf","executionInfo":{"status":"ok","timestamp":1748346791054,"user_tz":-360,"elapsed":200,"user":{"displayName":"Md. Atik Shahriar","userId":"05541666036680278842"}},"outputId":"34c3f787-f947-4afa-fda9-6cb01c5bee49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["34.141.193.105"]}]},{"cell_type":"code","source":["# New Cell in your Colab Notebook\n","\n","# Install localtunnel if not already installed in dependencies cell\n","!npm install localtunnel\n","\n","# Assuming streamlit_app.py is in your project_path\n","import os\n","project_path = '/content/drive/My Drive/SpecSentinel' # Make sure this path is correct\n","streamlit_script_path = os.path.join(project_path, 'streamlit_app.py')\n","\n","# Change directory to the project path so streamlit can find the script easily\n","%cd /content/drive/My Drive/SpecSentinel/\n","\n","# Run Streamlit in the background and expose it via localtunnel\n","!nohup env STREAMLIT_SERVER_FILE_WATCHER_TYPE=\"none\" streamlit run \"{streamlit_script_path}\" & npx localtunnel --port 8501 &\n","# You should see a public URL generated by localtunnel that you can open in your browser.\n","# To stop Streamlit, you might need to find its process ID and kill it.\n","# Alternatively, restart the Colab runtime."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZU-TGGKnSSWp","executionInfo":{"status":"ok","timestamp":1748347805082,"user_tz":-360,"elapsed":116124,"user":{"displayName":"Md. Atik Shahriar","userId":"05541666036680278842"}},"outputId":"d23a4c66-179f-4f38-e160-4e5ca030cda7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K\n","up to date, audited 23 packages in 854ms\n","\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K\n","\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K3 packages are looking for funding\n","\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K  run `npm fund` for details\n","\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K\n","2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n","\n","To address all issues (including breaking changes), run:\n","  npm audit fix --force\n","\n","Run `npm audit` for details.\n","\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K/content/drive/My Drive/SpecSentinel\n","nohup: appending output to 'nohup.out'\n","\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kyour url is: https://tricky-flies-fly.loca.lt\n"]}]},{"cell_type":"code","source":["# Cell to view the full content of nohup.out\n","!cat '/content/drive/My Drive/SpecSentinel/nohup.out'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BfHEdpwLaB-l","executionInfo":{"status":"ok","timestamp":1748347452488,"user_tz":-360,"elapsed":126,"user":{"displayName":"Md. Atik Shahriar","userId":"05541666036680278842"}},"outputId":"b67c4747-8330-464f-c512-aece7a0004ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-05-27 12:03:21.524 \"server.fileWatcher\" is not a valid config option. If you previously had this config option set, it may have been removed.\n","\n","Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n","\n","\n","  You can now view your Streamlit app in your browser.\n","\n","  Local URL: http://localhost:8501\n","  Network URL: http://172.28.0.12:8501\n","  External URL: http://34.141.193.105:8501\n","\n","  Stopping...\n"]}]},{"cell_type":"code","source":["# Make sure you are in the repository directory\n","%cd /content/drive/My Drive/SpecSentinel\n","\n","# If you haven't initialized a repository yet, uncomment this\n","# !git init\n","\n","# Configure user\n","!git config user.email \"atik.shahriar@dsinnovators.com\"\n","!git config user.name \"Md. Atik Shahriar\"\n","\n","# Check if the remote origin exists. If not, add it.\n","# If it exists, you can skip this or update it if needed.\n","# !git remote add origin \"https://github.com/mdatikshahriar/SpecSentinel.git\"\n","# If you get an error that origin already exists, you can remove it first:\n","# !git remote remove origin\n","# Then add it again:\n","# !git remote add origin \"https://github.com/mdatikshahriar/SpecSentinel.git\"\n","\n","# Move the notebook into the repository directory if it's not already there\n","!mv \"/content/drive/My Drive/Colab Notebooks/SpecSentinel-Main.ipynb\" \"/content/drive/My Drive/SpecSentinel/SpecSentinel-Main.ipynb\"\n","\n","# Add the notebook and the SpecSentinel folder (relative to the current directory)\n","!git add SpecSentinel-Main.ipynb\n","!git add . # This will add everything in the current directory (SpecSentinel/)\n","\n","# Commit your changes\n","!git commit -m \"Add notebook and SpecSentinel folder\"\n","\n","# Push to GitHub using the PAT from Colab Secrets\n","from google.colab import userdata\n","github_pat = userdata.get('GITHUB_PAT')\n","!git push https://mdatikshahriar:{github_pat}@github.com/mdatikshahriar/SpecSentinel.git master"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzx6WopZesTw","executionInfo":{"status":"ok","timestamp":1748348536092,"user_tz":-360,"elapsed":1416,"user":{"displayName":"Md. Atik Shahriar","userId":"05541666036680278842"}},"outputId":"d80e9191-5623-4ed2-eed7-4826081b3cdf"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/SpecSentinel\n","Reinitialized existing Git repository in /content/drive/My Drive/SpecSentinel/.git/\n","error: remote origin already exists.\n","fatal: /content/drive/My Drive/Colab Notebooks/SpecSentinel-Main.ipynb: '/content/drive/My Drive/Colab Notebooks/SpecSentinel-Main.ipynb' is outside repository at '/content/drive/My Drive/SpecSentinel'\n","On branch master\n","nothing to commit, working tree clean\n","fatal: could not read Username for 'https://github.com': No such device or address\n"]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMK0L2fRt5hI34ovJZcFo60"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}